> dat <- na.omit(read.csv("mice_pheno.csv"))
> head(dat)
  Sex Diet Bodyweight
1   F   hf      31.94
2   F   hf      32.48
3   F   hf      22.82
4   F   hf      19.92
5   F   hf      32.22
6   F   hf      27.50

 ##If a list of numbers has a distribution that is well approximated by the normal distribution, what proportion of these numbers are within one standard deviation away from the list’s average? 


 ## 68% 


 ** What proportion of these numbers are within two standard deviations away from the list’s average? ** 


 ## 95% 


 **What proportion of these numbers are within three standard deviations away from the list’s average? ** 


 ## 99.7% 


 ** Define y to be the weights of males on the control diet. What proportion of the mice are within one standard deviation away from the average weight (remember to use popsd for the population sd) ** 

> y <- filter(dat, Sex == "M" & Diet == "chow") %>% select(Bodyweight) %>% 
+ unlist()
> head(y)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      45.23       23.73       34.03       24.98       28.56       29.32 

 ## use popsd() to calculate 1sd if we have the entire population (mice_pheno.csv is full dataset), otherwise use sd() if we have sample of a larger population 

> library(rafalib)
> popsd(y)
[1] 4.420501

 ## 1 sd = 4.420501 

> mean(y)
[1] 30.96381

 ## average male control diet bodyweight = 30.96381 


 ## filter the data values that are +- 4.420501 away from the mean. This selects mice bodyweight values that fall within this range. calculate what % of mice fall in this interval compared to the total with mean() 

> m <- mean(y)
> n <- popsd(y)
> mean(dat >= m - n & dat <= m + n)
[1] 0.1458581
> mean(y >= m - n & dat <= m + n)
[1] 0.2358304
> mean(y >= m - n & y <= m + n)
[1] 0.6950673

 ## refer to filtered male, control diet mice variable *y*, not unfiltered dataset *dat* 


 ##i.e. 69.5% of male, control diet mice bodyweight values fall within 1 sd range 


 ** What proportion of these numbers are within two standard deviations away from the list’s average? ** 

> n <- popsd(y) * 2

 ## i.e. popsd() returns 1sd value, therefore *2 gives 2sd values 

> `?`(popsd())
> mean(y >= m - n & y <= m + n)
[1] 0.9461883

 ## 94.6% of male control diet bodyweight values fall within 2 sd from the mean 


 **What proportion of these numbers are within three standard deviations away from the list’s average? ** 

> n <- popsd(y) * 3
> mean(y >= m - n & y <= m + n)
[1] 0.9910314

 ## 99/1% 


 ** Note that the numbers for the normal distribution and our weights are relatively close. Also, notice that we are indirectly comparing quantiles of the normal distribution to quantiles of the mouse weight distribution. We can actually compare all quantiles using a qqplot. Which of the following best describes the qq-plot comparing mouse weights to the normal distribution? ** 


 ##qqnorm to compare against normal distribution (CDF), qqplot to compare between 2 datasets (we only have 1 i.e. y 

> `?`(qqnorm)
> qqnorm(y)
> qqline(y)
> `?`(qwnorm)
No documentation for ‘qwnorm’ in specified packages and libraries:
you could try ‘??qwnorm’
> `?`(qqnorm)

 ##C) The mouse weights are well approximated by the normal distribution, although the larger values (right tail) are larger than predicted by the normal. This is consistent with the differences seen between question 3 and 6 


 ## i.e. values on right tail are larger than expected (if assuming normal distribution). The sd calculations above show that y dataset is very close to a normal distribution, but with more sd away (2 and 3 sd), it is further from the expected/theoretical normal distribution. i.e. 3sd CDF = 99.7%, but y dataset is 99.1%, i.e. less than expected data falls within the 3sd range (bc of extreme values, hence the deviating values from the right tail) 


 **Create the above qq-plot for the four populations: male/females on each of the two diets. What is the most likely explanation for the mouse weights being well approximated? What is the best explanation for all these being well approximated by the normal distribution? ** 

> z <- filter(dat, Sex == "F" & Diet == "chow") %>% select(Bodyweight) %>% 
+ unlist()
> head(z)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      27.03       24.80       27.02       28.07       23.55       22.72 
> qqline(z)
> qqnorm(z)
> qqline(z)

 ## female control diet mice are also close to normal distibution. Some extreme values on the tails. Appears that right tail (larger values) are more extreme in female control diet compared to male control diet 

> qqnorm(y)
> qqline(y)
> qqnorm(z)
> qqline(y)
> qqline(z)
> yhf <- filter(dat, Sex == "M" & Diet == "hf") %>% select(Bodyweight)
> yhf <- unlist(yhf)
> head(yhf)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      27.81       29.45       35.11       31.25       38.26       29.06 
> qqnorm(yhf)
> zhf <- filter(dat, Sex == "F" & Diet == "hf") %>% select(Bodyweight) %>% 
+ unlist()
> head(zhf)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      31.94       32.48       22.82       19.92       32.22       27.50 
> qqnorm(zhf)
> qqline(zhf)

 ## ANS: A The CLT tells us that sample averages are approximately normal. 


 ** Here we are going to use the function replicate to learn about the distribution of random variables. All the above exercises relate to the normal distribution as an approximation of the distribution of a fixed list of numbers or a population. We have not yet discussed probability in these exercises. If the distribution of a list of numbers is approximately normal, then if we pick a number at random from this distribution, it will follow a normal distribution. However, it is important to remember that stating that some quantity has a distribution does not necessarily imply this quantity is random. Also, keep in mind that this is not related to the central limit theorem. The central limit applies to averages of random variables. Let’s explore this concept. ** 

> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7fb07502d3c0>
<environment: namespace:TeachingDemos>
txt> txtStart("Ch1_Inference.txt", append = TRUE)
txt> `?`(sample)
txt> `?`(sample)
txt> `?`(replicate)
txt> `?`(replicate)
txt> set.seed(1)
txt> set.seed(1)
txt> avgs <- sample(y, 25) %>% mean() %>% replicate(1000)
txt> avgs <- sample(y, 25) %>% mean() %>% replicate(1000)
txt> head(avgs)
[1] 1000 1000 1000 1000 1000 1000
txt> head(avgs)
txt> avgs <- replicate(1000, mean(sample(y, 25)))
txt> avgs <- replicate(1000, mean(sample(y, 25)))
txt> head(avgs)
[1] 31.2988 31.1696 29.3772 31.9388 31.7060 29.7660
txt> head(avgs)

 ##`replicate() peforms the Monte Carlo simulation from earlier in the chapter. Take a sample of size 25 from male control diet mice (y), use mean() to get 1000 values of the average of each sample group, replicate 1000x to get 1000 averages 

txt> txtComment("##`replicate() peforms the Monte Carlo simulation from earlier in the chapter. Take a sample of size 25 from male control diet mice (y), use mean() to get 1000 values of the average of each sample group, replicate 1000x to get 1000 averages")
txt> avgs <- replicate(10000, mean(sample(y, 25)))
txt> avgs <- replicate(10000, mean(sample(y, 25)))
txt> hist(avgs)
txt> hist(avgs)
txt> qqnorm(avgs)
txt> qqnorm(avgs)
txt> qqline(avgs)
txt> qqline(avgs)
txt> `?`(sd)
txt> `?`(sd)
txt> sd(avgs)
[1] 0.8268776
txt> sd(avgs)
txt> set.seed(1)
txt> set.seed(1)
txt> avgs <- replicate(10000, mean(sample(y, 25)))
txt> avgs <- replicate(10000, mean(sample(y, 25)))
txt> hist(avgs)
txt> hist(avgs)
txt> qqnorm(avgs)
txt> qqnorm(avgs)
txt> qqline(avgs)
txt> qqline(avgs)
txt> sd(avgs)
[1] 0.8271233
txt> sd(avgs)
txt> mean(y)
[1] 30.96381
txt> mean(y)
txt> mean(avgs)
[1] 30.96856
txt> mean(avgs)
txt> `?`(sqrt)
txt> `?`(sqrt)
txt> `?`(popsd)
txt> `?`(popsd)
txt> popsd(avgs)
[1] 0.827082
txt> popsd(avgs)

 **According to the CLT, the answer to exercise 9 should be the same as mean(y). You should be able to confirm that these two numbers are very close. Which of the following does the CLT tell us should be close to your answer to exercise 10? 

txt> txtComment("**According to the CLT, the answer to exercise 9 should be the same as mean(y). You should be able to confirm that these two numbers are very close. Which of the following does the CLT tell us should be close to your answer to exercise 10?")

 ## CLT central limit theorem calculates the standard deviation of a group of sample averages by taking the population sd (popsd()) and dividing it by the square root of n (sampel size) 

txt> txtComment("## CLT central limit theorem calculates the standard deviation of a group of sample averages by taking the population sd (popsd()) and dividing it by the square root of n (sampel size)")

 ## therfore ANS = D) popsd(y)/sqrt(25) 

txt> txtComment("## therfore ANS = D) popsd(y)/sqrt(25)")

 ** (popsd(y)) which is why we can’t use the CLT directly. This is because we see a sample and not the entire distribution. We also can’t use popsd(avgs) because to construct averages, we have to take 10,000 samples and this is never practical. We usually just get one sample. Instead we have to estimate popsd(y). As described, what we use is the sample standard deviation. Set the seed at 1, using the replicate function, create 10,000 samples of 25 and now, instead of the sample average, keep the standard deviation. Look at the distribution of the sample standard deviations. It is a random variable. The real population SD is about 4.5. What proportion of the sample SDs are below 3.5? ** 

txt> txtComment("** (popsd(y)) which is why we can’t use the CLT directly. This is because we see a sample and not the entire distribution. We also can’t use popsd(avgs) because to construct averages, we have to take 10,000 samples and this is never practical. We usually just get one sample. Instead we have to estimate popsd(y). As described, what we use is the sample standard deviation. Set the seed at 1, using the replicate function, create 10,000 samples of 25 and now, instead of the sample average, keep the standard deviation. Look at the distribution of the sample standard deviations. It is a random variable. The real population SD is about 4.5. What proportion of the sample SDs are below 3.5? **")
txt> set.seed(1)
txt> set.seed(1)
txt> sds <- replicate(10000, sd(sample(y, 25)))
txt> sds <- replicate(10000, sd(sample(y, 25)))
txt> head(sds)
[1] 3.928546 5.454165 3.838789 3.838535 4.997106 4.898378
txt> head(sds)

 ## i.e. replacing mean() with sds() means for each value generated in the samples of group size 25, its standard deviation from the average/mean of dataset y is generated 

txt> txtComment("## i.e. replacing mean() with sds() means for each value generated in the samples of group size 25, its standard deviation from the average/mean of dataset y is generated")

 ## these are classed as random variables because the values generated are reliant on the random nature of sample generation. It wouldn't produce the same sets of sd values if we ran again w/ different seed 

txt> txtComment("## these are classed as random variables because the values generated are reliant on the random nature of sample generation. It wouldn't produce the same sets of sd values if we ran again w/ different seed")
txt> mean(sds <= 3.5)
[1] 0.0942
txt> mean(sds <= 3.5)

 ## 9% of values in male control diet mice bodyweight values are 3.5 sd below the mean 

txt> txtComment("## 9% of values in male control diet mice bodyweight values are 3.5 sd below the mean")

 ## if the real population is ~4.5, it would make sense that few (9%) of data is not 4.5 sd (?) 

txt> txtComment("## if the real population is ~4.5, it would make sense that few (9%) of data is not 4.5 sd (?)")
txt> sds <- replicate(10000, popsd(sample(y, 25)))
txt> sds <- replicate(10000, popsd(sample(y, 25)))
txt> head(sds)
[1] 4.016802 3.742225 4.033065 3.402931 4.143360 4.041522
txt> head(sds)
txt> mean(sds <= 3.5)
[1] 0.1108
txt> mean(sds <= 3.5)

 ## actaully use popsd()? proportion is 11% 

txt> txtComment("## actaully use popsd()? proportion is 11%")
txt> `?`(qt)
txt> `?`(qt)
txt> `?`(seq)
txt> `?`(seq)

 **What the answer to question 12 reveals is that the denominator of the t-test is a random variable. By decreasing the sample size, you can see how this variability can increase. It therefore adds variability. The smaller the sample size, the more variability is added. The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which of the following is true? ** 

txt> txtComment("**What the answer to question 12 reveals is that the denominator of the t-test is a random variable. By decreasing the sample size, you can see how this variability can increase. It therefore adds variability. The smaller the sample size, the more variability is added. The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which of the following is true? **")
txt> x = seq(1e-04, 0.9999, len = 300)
txt> x = seq(1e-04, 0.9999, len = 300)
txt> head(x)
[1] 0.000100000 0.003443813 0.006787625 0.010131438 0.013475251 0.016819064
txt> head(x)

 i.e. len=300 means 300 values, starting from 0.0001 up to 0.9999, in equal increments 

txt> txtComment("i.e. len=300 means 300 values, starting from 0.0001 up to 0.9999, in equal increments")
txt> dim(x)
NULL
txt> dim(x)
txt> length(x)
[1] 300
txt> length(x)
txt> `?`(qt)
txt> `?`(qt)
txt> qt(x, 3)
  [1] -22.203742273  -6.664348674  -5.233577197  -4.518726011  -4.059172734  -3.727266670  -3.470704908
  [8]  -3.263323229  -3.090301354  -2.942494303  -2.813893155  -2.700354695  -2.598910948  -2.507368823
 [15]  -2.424065585  -2.347712970  -2.277294188  -2.211993758  -2.151148443  -2.094212135  -2.040730233
 [22]  -1.990320584  -1.942659108  -1.897468777  -1.854511070  -1.813579263  -1.774493106  -1.737094556
 [29]  -1.701244333  -1.666819106  -1.633709186  -1.601816617  -1.571053586  -1.541341090  -1.512607813
 [36]  -1.484789176  -1.457826525  -1.431666443  -1.406260147  -1.381562978  -1.357533956  -1.334135392
 [43]  -1.311332548  -1.289093341  -1.267388085  -1.246189257  -1.225471300  -1.205210434  -1.185384506
 [50]  -1.165972841  -1.146956113  -1.128316238  -1.110036265  -1.092100286  -1.074493352  -1.057201399
 [57]  -1.040211180  -1.023510203  -1.007086672  -0.990929443  -0.975027971  -0.959372268  -0.943952869
 [64]  -0.928760792  -0.913787507  -0.899024906  -0.884465275  -0.870101269  -0.855925890  -0.841932461
 [71]  -0.828114612  -0.814466257  -0.800981578  -0.787655010  -0.774481227  -0.761455125  -0.748571813
 [78]  -0.735826597  -0.723214975  -0.710732619  -0.698375372  -0.686139236  -0.674020362  -0.662015047
 [85]  -0.650119720  -0.638330940  -0.626645390  -0.615059865  -0.603571273  -0.592176626  -0.580873033
 [92]  -0.569657702  -0.558527927  -0.547481089  -0.536514653  -0.525626159  -0.514813222  -0.504073527
 [99]  -0.493404829  -0.482804945  -0.472271754  -0.461803194  -0.451397259  -0.441051995  -0.430765503
[106]  -0.420535928  -0.410361464  -0.400240351  -0.390170868  -0.380151338  -0.370180121  -0.360255615
[113]  -0.350376253  -0.340540502  -0.330746862  -0.320993864  -0.311280067  -0.301604060  -0.291964459
[120]  -0.282359904  -0.272789060  -0.263250618  -0.253743289  -0.244265804  -0.234816916  -0.225395398
[127]  -0.216000040  -0.206629649  -0.197283049  -0.187959079  -0.178656593  -0.169374460  -0.160111560
[134]  -0.150866787  -0.141639045  -0.132427250  -0.123230327  -0.114047211  -0.104876846  -0.095718182
[141]  -0.086570179  -0.077431801  -0.068302020  -0.059179812  -0.050064158  -0.040954043  -0.031848455
[148]  -0.022746385  -0.013646827  -0.004548775   0.004548775   0.013646827   0.022746385   0.031848455
[155]   0.040954043   0.050064158   0.059179812   0.068302020   0.077431801   0.086570179   0.095718182
[162]   0.104876846   0.114047211   0.123230327   0.132427250   0.141639045   0.150866787   0.160111560
[169]   0.169374460   0.178656593   0.187959079   0.197283049   0.206629649   0.216000040   0.225395398
[176]   0.234816916   0.244265804   0.253743289   0.263250618   0.272789060   0.282359904   0.291964459
[183]   0.301604060   0.311280067   0.320993864   0.330746862   0.340540502   0.350376253   0.360255615
[190]   0.370180121   0.380151338   0.390170868   0.400240351   0.410361464   0.420535928   0.430765503
[197]   0.441051995   0.451397259   0.461803194   0.472271754   0.482804945   0.493404829   0.504073527
[204]   0.514813222   0.525626159   0.536514653   0.547481089   0.558527927   0.569657702   0.580873033
[211]   0.592176626   0.603571273   0.615059865   0.626645390   0.638330940   0.650119720   0.662015047
[218]   0.674020362   0.686139236   0.698375372   0.710732619   0.723214975   0.735826597   0.748571813
[225]   0.761455125   0.774481227   0.787655010   0.800981578   0.814466257   0.828114612   0.841932461
[232]   0.855925890   0.870101269   0.884465275   0.899024906   0.913787507   0.928760792   0.943952869
[239]   0.959372268   0.975027971   0.990929443   1.007086672   1.023510203   1.040211180   1.057201399
[246]   1.074493352   1.092100286   1.110036265   1.128316238   1.146956113   1.165972841   1.185384506
[253]   1.205210434   1.225471300   1.246189257   1.267388085   1.289093341   1.311332548   1.334135392
[260]   1.357533956   1.381562978   1.406260147   1.431666443   1.457826525   1.484789176   1.512607813
[267]   1.541341090   1.571053586   1.601816617   1.633709186   1.666819106   1.701244333   1.737094556
[274]   1.774493106   1.813579263   1.854511070   1.897468777   1.942659108   1.990320584   2.040730233
[281]   2.094212135   2.151148443   2.211993758   2.277294188   2.347712970   2.424065585   2.507368823
[288]   2.598910948   2.700354695   2.813893155   2.942494303   3.090301354   3.263323229   3.470704908
[295]   3.727266670   4.059172734   4.518726011   5.233577197   6.664348674  22.203742273
txt> qt(x, 3)
txt> qqnorm(x)
txt> qqnorm(x)
txt> qt(x, 3) %>% qqnorm()
txt> qt(x, 3) %>% qqnorm()
txt> qt(x, 10) %>% qqnorm()
txt> qt(x, 10) %>% qqnorm()
txt> qt(x, 30) %>% qqnorm()
txt> qt(x, 30) %>% qqnorm()
txt> qt(x, 100) %>% qqnorm()
txt> qt(x, 100) %>% qqnorm()

 ## the above exercise using replicate (Monte Carlo) leads to random variables. this means that the sd varies i.e. not fixed. with larger sample sizes, sd varies less. however, sd struggles with smaller samples i.e. will show larger variances. t-distribution with qt() takes into account the larger variability that sd struggles with in smaller samples by   

txt> txtComment("## the above exercise using replicate (Monte Carlo) leads to random variables. this means that the sd varies i.e. not fixed. with larger sample sizes, sd varies less. however, sd struggles with smaller samples i.e. will show larger variances. t-distribution with qt() takes into account the larger variability that sd struggles with in smaller samples by  ")

 ## elongating the tail ends, i.e. increasing the spread of the data to compensate for the higher variance. 

txt> txtComment("## elongating the tail ends, i.e. increasing the spread of the data to compensate for the higher variance.")

 ## and 'elongating the tails' means its making the distribution wider than normal to account/compensate for the larger variance seen in smaller samples 

txt> txtComment("## and 'elongating the tails' means its making the distribution wider than normal to account/compensate for the larger variance seen in smaller samples")

 ## the larger the sample, the less 'elongated tails' there are with t-distribution, because a larger sample means its closer to the population actual value, hence there's not much variability to account/compensate for, hence the more-linear curve 

txt> txtComment("## the larger the sample, the less 'elongated tails' there are with t-distribution, because a larger sample means its closer to the population actual value, hence there's not much variability to account/compensate for, hence the more-linear curve")

 ## the df (degrees of freedom) reflect this, df = sample sizes, the smaller the sample, the more elongated the tails are, i.e. qt() makes the distribution larger than normal to compensate for the higher variance in smaller samples 

txt> txtComment("## the df (degrees of freedom) reflect this, df = sample sizes, the smaller the sample, the more elongated the tails are, i.e. qt() makes the distribution larger than normal to compensate for the higher variance in smaller samples")
txt> qt(x, 3) %>% qqnorm()
txt> qt(x, 3) %>% qqnorm()
txt> qt(x, 10) %>% qqnorm()
txt> qt(x, 10) %>% qqnorm()
txt> qt(x, 30) %>% qqnorm()
txt> qt(x, 30) %>% qqnorm()
txt> qt(x, 100) %>% qqnorm()
txt> qt(x, 100) %>% qqnorm()

 ## ANS: C) The t-distribution has larger tails up until 30 degrees of freedom, at which point it is practically the same as the normal distribution. 

txt> txtComment("## ANS: C) The t-distribution has larger tails up until 30 degrees of freedom, at which point it is practically the same as the normal distribution.")
txt> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7fb07502d3c0>
<environment: namespace:TeachingDemos>
txt> txtStop
> library(rafalib)
> library(tidyverse)
> library(downloader)

 ## CLT: the more you sample and take its average, the more you cancel out / silence the extreme values. therefore CLT will tend towards a normal distribution bell curve if you take sample averages many times 

> dat <- read.csv("mice_pheno.csv")
> head(dat)
  Sex Diet Bodyweight
1   F   hf      31.94
2   F   hf      32.48
3   F   hf      22.82
4   F   hf      19.92
5   F   hf      32.22
6   F   hf      27.50
> controlPopulation <- filter(dat, Sex == "F" & Diet == "chow") %>% 
+ select(Bodyweight) %>% unlist()
> hfPopulation <- filter(dat, Sex == "F" & Diet == "hf") %>% select(Bodyweight) %>% 
+ unlist()
> sd_control <- popsd(controlPopulation)
> sd_hf <- popsd(controlPopulation)
> sd_control <- popsd(controlPopulation)
> sd_hf <- popsd(hfPopulation)
> `?`(function(N) n)
> Ns <- c(3, 12, 25, 50)
> B <- 10000
> res <- (sapply(Ns, function(n) {
+ replicate(B, mean(sample(hfPopulation, n)) - mean(sample(controlPopulation, 
+ 
+ n)))
+ }))

 ## i.e. sapply loops the following for each value in vector Ns. Ns holds sample size values - test to see how the sampling popsd values change depending on sample size 

> head(res)
          [,1]      [,2]   [,3]    [,4]
[1,] 3.4800000  1.975000 3.0380 2.27338
[2,] 1.5400000  3.928333 4.0436 4.36620
[3,] 3.4866667  3.211667 3.0672 2.71720
[4,] 0.4666667  3.135833 1.7496 3.00978
[5,] 5.5166667 -2.525000 1.8964 1.93980
[6,] 2.1633333  4.505000 2.9004 1.95480
> n <- 12
> head(res)
          [,1]      [,2]   [,3]    [,4]
[1,] 3.4800000  1.975000 3.0380 2.27338
[2,] 1.5400000  3.928333 4.0436 4.36620
[3,] 3.4866667  3.211667 3.0672 2.71720
[4,] 0.4666667  3.135833 1.7496 3.00978
[5,] 5.5166667 -2.525000 1.8964 1.93980
[6,] 2.1633333  4.505000 2.9004 1.95480

 ## function(n) opens loop to calculate the mean differences between control and hf diet mice bodyweight, repeated 10,000 times, for sample sizes listed in Ns: 3, 12, 25, 50 


 ## the 'n' inside sample(hfPopulation, n) is referring to the sample size as the loop cycles through each vector value in variable Ns e.g. 3 


 ## if the CLT is working, then the qq plots with larger sample sizes (i.e. n = 50) should have the most normal distribution i.e. closest to linear graph, i.e. reflecting that the larger the sample size, the more normal the distribution (because the extreme values are silenced, growing towards central tendency) 

> `?`(mypar)
> `?`(along)
> `?`(signif)

 ## the original population itself may already have a normal distribution, which will also be reflected in the sample CLT plots 

> `?`(var)

 ## variance of a sample mean (var(Y) is equal to the population variance, divided by the sample size (n); hence: variance of sample mean is equal to var(y) / n 


 ## when comparing the variance between 2 independent groups, you add their individual variances; i.e. the more groups you introduce, the larger the overall variance 


 ##square root that to get the standard error, i/e. sqrt(var(y)/n + var(x)/n) 


 ## then divide the mean difference between the sample averages of the 2 groups (numerator), by the above (denominator)^ 

> url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
> filename <- "femaleMiceWeights.csv"
> if (!file.exists("femaleMiceWeights.csv")) download(url, destfile = filename)
> if (!file.exists("femaleMiceWeights.csv")) download(url, destfile = filename)
> dat <- read.csv(filename)

 ** The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let’s try a simple example related to gambling. ** 


 ** Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies. ** 


 **. We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). ** 

> set.seed(1)

 ## variance = p(1-p)/n; where p = probabilty 


 ## i.e. p = 1/6 times we will roll a 6. subtract from 1 to get the number of times we will not get a 6 (5/6). multiply the failure probability (5/6) by our wanted probability (1/6). multiplying two probabilities together means you expect both outcomes at the same time, but in this case it would mean getting a value that explains what the overall *uncertainty* is to achieve the 1/6 times of rolling a 6. multipling a small fraction (1/6) by a larger fraction (5/6) would give a relatively low number, i.e. explaining low variance in the data - this is because we expect more 'No 


 ## if we had a higher chance of rolling a 6, e.g. a biased dice with a 3/6 likelihood of rolling a 6, 3/6 times the dice would not roll a 6. Multiplying these fractions would give a larger number, hence explaining a larger variance of data (Yes and No to rolling a 6) 


 ## finally, variance equation p(1-6)/n includes n for sample size - this takes the average probability of getting a 6 on the dice. the larger the n value, the more amount of samples were collected, i.e. more likely to silence out the 'noise' in the data, hence divindg variance by a large n will produce a smaller variance value 


 ## sqrt(p(1-p)/n) gives the standard error 

> `?`(replicate)
> set.seed(1)

 ** We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). **  

> x = sample(1:6, 10000, replace = TRUE)
> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7fac73c8f6d0>
<environment: namespace:TeachingDemos>
txt> txtStart("Ch1_Inference.txt", append = TRUE)

 ## variance p*(1-p)/n explains the spread of the data. dividing by the sample size explains the 'noise', i.e. the larger the sample, the larger the divisible factor, hence a lower variation. 

txt> txtComment("## variance p*(1-p)/n explains the spread of the data. dividing by the sample size explains the 'noise', i.e. the larger the sample, the larger the divisible factor, hence a lower variation.")

 ## variance units is sigma^2, i.e. squared to remove any negative values for a standardised variance value. 

txt> txtComment("## variance units is sigma^2, i.e. squared to remove any negative values for a standardised variance value.")

 ## therefore, sqrt(variance) gives the standard error, i.e. the value of 1 standard deviation is x amount from the population mean 

txt> txtComment("## therefore, sqrt(variance) gives the standard error, i.e. the value of 1 standard deviation is x amount from the population mean")

 ## the z-score standardises the whole data so that you can compare to other datasets. different datasets have different means and sd. z-score standardises sample data to mean = 0 and to 1 sd. this produces a bell curve / z-score that can be compared to other standardised datasets 

txt> txtComment("## the z-score standardises the whole data so that you can compare to other datasets. different datasets have different means and sd. z-score standardises sample data to mean = 0 and to 1 sd. this produces a bell curve / z-score that can be compared to other standardised datasets")

 ## (mean(x==6) - p) selects the number of times the die rolled a 6, divided by the total number of rolls in x dataset, i.e. the average number of times 6 was rolled. 

txt> txtComment("## (mean(x==6) - p) selects the number of times the die rolled a 6, divided by the total number of rolls in x dataset, i.e. the average number of times 6 was rolled.")

 ## (- p) subtracts the probability from the above mean value^ to shift the average mean value to 0, hence standardising the data 

txt> txtComment("## (- p) subtracts the probability from the above mean value^ to shift the average mean value to 0, hence standardising the data")

 ## CLT should be normal (as each die roll is independent) with mean 0 and SD 1. 'normal' i.e. standard bell curve and normal distribution, expected variation, silenced the noise (outliers) 

txt> txtComment("## CLT should be normal (as each die roll is independent) with mean 0 and SD 1. 'normal' i.e. standard bell curve and normal distribution, expected variation, silenced the noise (outliers)")

 ** We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). ** 

txt> txtComment("** We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). **")
txt> set.seed(1)
txt> set.seed(1)
txt> p = 1/6
txt> p = 1/6
txt> sample(1:6, replace = TRUE)
[1] 1 4 1 2 5 3
txt> sample(1:6, replace = TRUE)
txt> mean(sample(1:6, replace = TRUE))
[1] 3.333333
txt> mean(sample(1:6, replace = TRUE))
txt> mean(sample(1:6, replace = TRUE))
[1] 3.666667
txt> mean(sample(1:6, replace = TRUE))
txt> mean(sample(1:6, replace = TRUE))
[1] 3.833333
txt> mean(sample(1:6, replace = TRUE))

 ## the expected mean is around 3.5 for rolling a 6 sided dice. we need to replicate calculating the mean of a sample 10,000 times 

txt> txtComment("## the expected mean is around 3.5 for rolling a 6 sided dice. we need to replicate calculating the mean of a sample 10,000 times")
txt> x <- mean(sample(1:6, replace = TRUE))
txt> x <- mean(sample(1:6, replace = TRUE))
txt> x
[1] 3.333333
txt> x
txt> x
[1] 3.333333
txt> x
txt> x
[1] 3.333333
txt> x
txt> x = mean(sample(1:6, replace = TRUE))
txt> x = mean(sample(1:6, replace = TRUE))
txt> x
[1] 3
txt> x
txt> x
[1] 3
txt> x
txt> x
[1] 3
txt> x
txt> x <- replicate(10000, mean(sample(1:6, replace = TRUE)))
txt> x <- replicate(10000, mean(sample(1:6, replace = TRUE)))
txt> head(x)
[1] 4.000000 3.166667 3.500000 4.333333 4.166667 3.000000
txt> head(x)
txt> set.seed(1)
txt> set.seed(1)
txt> x <- replicate(10000, mean(sample(1:6, replace = TRUE)))
txt> x <- replicate(10000, mean(sample(1:6, replace = TRUE)))
txt> head(x)
[1] 2.666667 3.333333 3.666667 3.833333 3.333333 3.000000
txt> head(x)

 ## actualy, don't include mean in the replicate simulation yet, bc we need to do mean() to calculate the z-score. formula wants to select x == 6, but can't because x variable only has means / no 6 integer 

txt> txtComment("## actualy, don't include mean in the replicate simulation yet, bc we need to do mean() to calculate the z-score. formula wants to select x == 6, but can't because x variable only has means / no 6 integer")
txt> set.seed(1)
txt> set.seed(1)
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> head(x)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18]
[1,]    1    6    5    5    5    1    6    1    2     5     3     1     4     4     1     3     2     2
     [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34]
[1,]     4     3     4     1     3     6     3     4     2     3     1     5     6     1     5     1
     [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]
[1,]     4     5     5     5     4     6     3     5     5     4     1     6     2     2     3     6
     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66]
[1,]     5     5     3     5     1     5     2     5     3     5     2     3     6     2     1     1
     [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82]
[1,]     6     2     1     2     2     5     1     6     6     1     4     4     2     3     3     6
     [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]
[1,]     2     4     6     2     2     5     3     1     3     4     3     1     1     2     3     1
     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108] [,109] [,110] [,111] [,112]
[1,]     2      5      5      1      4      1      4      2      6      6      5      6      3      6
     [,113] [,114] [,115] [,116] [,117] [,118] [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126]
[1,]      4      3      6      6      4      5      1      2      4      5      2      2      5      5
     [,127] [,128] [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138] [,139] [,140]
[1,]      4      1      3      4      2      5      1      4      2      5      2      3      2      1
     [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148] [,149] [,150] [,151] [,152] [,153] [,154]
[1,]      4      2      6      4      4      5      3      2      6      5      3      6      2      4
     [,155] [,156] [,157] [,158] [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]
[1,]      6      5      2      6      5      2      5      2      5      1      1      6      5      6
     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178] [,179] [,180] [,181] [,182]
[1,]      3      6      4      6      5      5      1      6      3      3      6      2      2      1
     [,183] [,184] [,185] [,186] [,187] [,188] [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196]
[1,]      6      6      4      1      5      5      6      4      3      6      4      4      3      2
     [,197] [,198] [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208] [,209] [,210]
[1,]      2      5      1      4      6      1      2      5      3      1      1      6      2      5
     [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218] [,219] [,220] [,221] [,222] [,223] [,224]
[1,]      2      3      3      2      1      2      4      2      6      1      4      1      2      3
     [,225] [,226] [,227] [,228] [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]
[1,]      4      5      3      4      4      6      3      5      3      1      5      1      4      6
     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248] [,249] [,250] [,251] [,252]
[1,]      6      6      6      4      5      1      1      6      1      6      1      6      3      3
     [,253] [,254] [,255] [,256] [,257] [,258] [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266]
[1,]      4      2      2      3      3      1      2      5      3      5      6      5      5      3
     [,267] [,268] [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278] [,279] [,280]
[1,]      5      5      5      5      6      6      2      2      6      3      5      1      5      5
     [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288] [,289] [,290] [,291] [,292] [,293] [,294]
[1,]      2      6      4      4      2      2      2      6      1      2      5      3      4      3
     [,295] [,296] [,297] [,298] [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]
[1,]      5      4      1      2      3      6      2      2      5      1      4      2      5      5
     [,309] [,310] [,311] [,312] [,313] [,314] [,315] [,316] [,317] [,318] [,319] [,320] [,321] [,322]
[1,]      6      6      6      6      2      1      4      3      2      1      6      5      6      5
     [,323] [,324] [,325] [,326] [,327] [,328] [,329] [,330] [,331] [,332] [,333] [,334] [,335] [,336]
[1,]      5      6      2      2      2      2      5      3      4      4      4      1      1      4
     [,337] [,338] [,339] [,340] [,341] [,342] [,343] [,344] [,345] [,346] [,347] [,348] [,349] [,350]
[1,]      5      4      3      3      1      2      2      5      3      3      4      3      1      6
     [,351] [,352] [,353] [,354] [,355] [,356] [,357] [,358] [,359] [,360] [,361] [,362] [,363] [,364]
[1,]      4      2      3      2      1      5      5      3      3      3      2      4      6      4
     [,365] [,366] [,367] [,368] [,369] [,370] [,371] [,372] [,373] [,374] [,375] [,376] [,377] [,378]
[1,]      6      5      6      6      3      4      6      3      6      1      3      6      2      2
     [,379] [,380] [,381] [,382] [,383] [,384] [,385] [,386] [,387] [,388] [,389] [,390] [,391] [,392]
[1,]      3      4      4      2      2      6      4      5      1      4      2      2      2      6
     [,393] [,394] [,395] [,396] [,397] [,398] [,399] [,400] [,401] [,402] [,403] [,404] [,405] [,406]
[1,]      2      2      5      2      5      5      2      1      6      2      4      4      6      4
     [,407] [,408] [,409] [,410] [,411] [,412] [,413] [,414] [,415] [,416] [,417] [,418] [,419] [,420]
[1,]      4      4      5      1      4      5      5      6      1      3      6      4      2      5
     [,421] [,422] [,423] [,424] [,425] [,426] [,427] [,428] [,429] [,430] [,431] [,432] [,433] [,434]
[1,]      4      3      5      2      1      5      5      2      2      1      6      5      2      2
     [,435] [,436] [,437] [,438] [,439] [,440] [,441] [,442] [,443] [,444] [,445] [,446] [,447] [,448]
[1,]      2      4      3      1      1      5      4      6      5      6      3      4      4      4
     [,449] [,450] [,451] [,452] [,453] [,454] [,455] [,456] [,457] [,458] [,459] [,460] [,461] [,462]
[1,]      4      4      5      3      3      1      2      5      6      6      3      2      4      4
     [,463] [,464] [,465] [,466] [,467] [,468] [,469] [,470] [,471] [,472] [,473] [,474] [,475] [,476]
[1,]      5      3      4      2      5      5      3      1      4      1      5      1      1      6
     [,477] [,478] [,479] [,480] [,481] [,482] [,483] [,484] [,485] [,486] [,487] [,488] [,489] [,490]
[1,]      2      5      5      6      2      2      5      4      5      1      6      5      2      4
     [,491] [,492] [,493] [,494] [,495] [,496] [,497] [,498] [,499] [,500] [,501] [,502] [,503] [,504]
[1,]      1      4      1      3      5      3      3      4      5      6      4      3      3      5
     [,505] [,506] [,507] [,508] [,509] [,510] [,511] [,512] [,513] [,514] [,515] [,516] [,517] [,518]
[1,]      6      6      4      2      2      6      1      3      1      5      1      5      4      4
     [,519] [,520] [,521] [,522] [,523] [,524] [,525] [,526] [,527] [,528] [,529] [,530] [,531] [,532]
[1,]      6      5      3      3      1      3      4      3      6      6      1      5      6      5
     [,533] [,534] [,535] [,536] [,537] [,538] [,539] [,540] [,541] [,542] [,543] [,544] [,545] [,546]
[1,]      6      6      5      5      3      1      6      4      3      1      1      5      5      5
     [,547] [,548] [,549] [,550] [,551] [,552] [,553] [,554] [,555] [,556] [,557] [,558] [,559] [,560]
[1,]      4      1      5      4      6      2      1      5      3      2      2      2      3      6
     [,561] [,562] [,563] [,564] [,565] [,566] [,567] [,568] [,569] [,570] [,571] [,572] [,573] [,574]
[1,]      2      1      2      4      3      5      5      5      4      6      1      6      5      1
     [,575] [,576] [,577] [,578] [,579] [,580] [,581] [,582] [,583] [,584] [,585] [,586] [,587] [,588]
[1,]      3      4      4      3      5      3      1      6      1      4      5      4      6      2
     [,589] [,590] [,591] [,592] [,593] [,594] [,595] [,596] [,597] [,598] [,599] [,600] [,601] [,602]
[1,]      5      1      6      4      3      1      4      3      4      2      1      2      1      4
     [,603] [,604] [,605] [,606] [,607] [,608] [,609] [,610] [,611] [,612] [,613] [,614] [,615] [,616]
[1,]      4      2      2      1      5      6      6      1      4      1      2      4      2      1
     [,617] [,618] [,619] [,620] [,621] [,622] [,623] [,624] [,625] [,626] [,627] [,628] [,629] [,630]
[1,]      2      2      6      4      4      3      6      5      3      6      1      6      1      2
     [,631] [,632] [,633] [,634] [,635] [,636] [,637] [,638] [,639] [,640] [,641] [,642] [,643] [,644]
[1,]      5      4      2      1      3      5      4      6      2      2      4      4      3      3
     [,645] [,646] [,647] [,648] [,649] [,650] [,651] [,652] [,653] [,654] [,655] [,656] [,657] [,658]
[1,]      4      4      5      3      1      3      5      5      2      1      5      1      1      1
     [,659] [,660] [,661] [,662] [,663] [,664] [,665] [,666] [,667] [,668] [,669] [,670] [,671] [,672]
[1,]      1      1      5      5      3      6      6      5      4      6      3      5      1      6
     [,673] [,674] [,675] [,676] [,677] [,678] [,679] [,680] [,681] [,682] [,683] [,684] [,685] [,686]
[1,]      2      3      2      1      6      5      4      2      6      2      3      2      5      6
     [,687] [,688] [,689] [,690] [,691] [,692] [,693] [,694] [,695] [,696] [,697] [,698] [,699] [,700]
[1,]      1      4      4      2      2      2      2      4      1      3      5      3      6      3
     [,701] [,702] [,703] [,704] [,705] [,706] [,707] [,708] [,709] [,710] [,711] [,712] [,713] [,714]
[1,]      6      2      4      4      4      5      2      1      4      2      1      4      2      1
     [,715] [,716] [,717] [,718] [,719] [,720] [,721] [,722] [,723] [,724] [,725] [,726] [,727] [,728]
[1,]      1      1      2      6      1      3      6      5      3      3      6      4      2      4
     [,729] [,730] [,731] [,732] [,733] [,734] [,735] [,736] [,737] [,738] [,739] [,740] [,741] [,742]
[1,]      1      1      4      6      2      3      4      4      1      4      3      3      5      4
     [,743] [,744] [,745] [,746] [,747] [,748] [,749] [,750] [,751] [,752] [,753] [,754] [,755] [,756]
[1,]      1      2      5      4      2      4      4      4      6      6      2      5      5      5
     [,757] [,758] [,759] [,760] [,761] [,762] [,763] [,764] [,765] [,766] [,767] [,768] [,769] [,770]
[1,]      1      5      1      3      2      1      2      3      3      5      6      4      5      2
     [,771] [,772] [,773] [,774] [,775] [,776] [,777] [,778] [,779] [,780] [,781] [,782] [,783] [,784]
[1,]      2      2      5      6      1      3      3      6      6      3      1      1      3      3
     [,785] [,786] [,787] [,788] [,789] [,790] [,791] [,792] [,793] [,794] [,795] [,796] [,797] [,798]
[1,]      5      5      2      1      6      4      5      3      2      2      6      1      3      1
     [,799] [,800] [,801] [,802] [,803] [,804] [,805] [,806] [,807] [,808] [,809] [,810] [,811] [,812]
[1,]      5      4      3      2      1      3      6      2      3      1      3      4      2      6
     [,813] [,814] [,815] [,816] [,817] [,818] [,819] [,820] [,821] [,822] [,823] [,824] [,825] [,826]
[1,]      6      3      1      6      3      2      4      6      1      5      1      3      2      2
     [,827] [,828] [,829] [,830] [,831] [,832] [,833] [,834] [,835] [,836] [,837] [,838] [,839] [,840]
[1,]      2      1      5      2      1      6      3      6      3      1      6      1      1      6
     [,841] [,842] [,843] [,844] [,845] [,846] [,847] [,848] [,849] [,850] [,851] [,852] [,853] [,854]
[1,]      5      3      4      1      2      6      4      4      6      5      1      2      2      4
     [,855] [,856] [,857] [,858] [,859] [,860] [,861] [,862] [,863] [,864] [,865] [,866] [,867] [,868]
[1,]      2      5      4      1      5      2      6      2      5      4      1      3      4      4
     [,869] [,870] [,871] [,872] [,873] [,874] [,875] [,876] [,877] [,878] [,879] [,880] [,881] [,882]
[1,]      4      3      1      3      6      6      2      1      1      4      6      2      2      4
     [,883] [,884] [,885] [,886] [,887] [,888] [,889] [,890] [,891] [,892] [,893] [,894] [,895] [,896]
[1,]      2      2      2      5      6      6      2      4      4      1      1      5      1      3
     [,897] [,898] [,899] [,900] [,901] [,902] [,903] [,904] [,905] [,906] [,907] [,908] [,909] [,910]
[1,]      4      3      5      5      1      2      4      6      1      2      3      1      5      1
     [,911] [,912] [,913] [,914] [,915] [,916] [,917] [,918] [,919] [,920] [,921] [,922] [,923] [,924]
[1,]      4      1      6      5      3      1      5      4      1      2      2      2      2      5
     [,925] [,926] [,927] [,928] [,929] [,930] [,931] [,932] [,933] [,934] [,935] [,936] [,937] [,938]
[1,]      5      5      2      4      6      4      5      4      4      1      6      2      6      5
     [,939] [,940] [,941] [,942] [,943] [,944] [,945] [,946] [,947] [,948] [,949] [,950] [,951] [,952]
[1,]      5      5      3      4      1      1      2      4      6      6      2      3      6      5
     [,953] [,954] [,955] [,956] [,957] [,958] [,959] [,960] [,961] [,962] [,963] [,964] [,965] [,966]
[1,]      4      1      6      5      1      2      6      4      3      2      5      1      2      2
     [,967] [,968] [,969] [,970] [,971] [,972] [,973] [,974] [,975] [,976] [,977] [,978] [,979] [,980]
[1,]      4      1      6      6      5      1      2      5      6      3      3      4      3      3
     [,981] [,982] [,983] [,984] [,985] [,986] [,987] [,988] [,989] [,990] [,991] [,992] [,993] [,994]
[1,]      4      3      3      1      4      2      6      2      1      1      3      5      6      6
     [,995] [,996] [,997] [,998] [,999] [,1000]
[1,]      3      4      1      5      3       3
 [ reached 'max' / getOption("max.print") -- omitted 5 rows and 9000 columns ]
txt> head(x)

 ## now raw values can be used to calculate mean in the z score formula 

txt> txtComment("## now raw values can be used to calculate mean in the z score formula")
txt> z <- (mean(x == 6) - p)/sqrt(p * (1 - p)/10000)
txt> z <- (mean(x == 6) - p)/sqrt(p * (1 - p)/10000)
txt> z
[1] -0.008944272
txt> z

 ## mean() is another way of defining 'proportion' bc it's the wanted value divided by the overall number of values in the dataset. same with p: 1 divided by 6 total numbers in the dice. by subtracting p from x, you are *comparing how far the observed proportion/probability is from the expected proportion/probability* 

txt> txtComment("## mean() is another way of defining 'proportion' bc it's the wanted value divided by the overall number of values in the dataset. same with p: 1 divided by 6 total numbers in the dice. by subtracting p from x, you are *comparing how far the observed proportion/probability is from the expected proportion/probability*")

 ## therefore, our observed probability was slightly less than the expected mean of 1/6 (0.16666), hence the negative z score 

txt> txtComment("## therefore, our observed probability was slightly less than the expected mean of 1/6 (0.16666), hence the negative z score")

 **and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05)** 

txt> txtComment("**and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05)**")
txt> mean(z > 2)
[1] 0
txt> mean(z > 2)
txt> head(z)
[1] -0.008944272
txt> head(z)
txt> x <- sample(1:6, 10000, replace = TRUE)
txt> x <- sample(1:6, 10000, replace = TRUE)
txt> head(x)
[1] 5 2 4 4 1 6
txt> head(x)
txt> set.seed(1)
txt> set.seed(1)
txt> x <- sample(1:6, 10000, replace = TRUE)
txt> x <- sample(1:6, 10000, replace = TRUE)
txt> head(x)
[1] 1 4 1 2 5 3
txt> head(x)
txt> head(x)
[1] 1 4 1 2 5 3
txt> head(x)
txt> x = sample(1:6, 10000, replace = TRUE)
txt> x = sample(1:6, 10000, replace = TRUE)
txt> head(x)
[1] 1 6 1 4 1 3
txt> head(x)
txt> head(x)
[1] 1 6 1 4 1 3
txt> head(x)
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> head(x)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18]
[1,]    5    6    3    4    1    6    5    5    4     1     2     1     4     6     6     3     3     1
     [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34]
[1,]     6     1     4     5     2     5     1     5     3     3     2     3     6     2     3     1
     [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]
[1,]     2     2     4     6     1     2     5     1     5     4     4     4     4     2     5     4
     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66]
[1,]     6     2     3     1     1     6     6     1     2     4     3     6     1     6     2     1
     [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82]
[1,]     5     3     1     3     2     1     6     5     5     2     1     4     5     6     1     1
     [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]
[1,]     4     3     2     4     4     1     5     2     5     4     2     2     1     5     1     1
     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108] [,109] [,110] [,111] [,112]
[1,]     2      6      6      4      3      6      3      3      5      2      2      6      5      3
     [,113] [,114] [,115] [,116] [,117] [,118] [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126]
[1,]      1      6      5      6      3      2      6      5      4      2      6      2      2      6
     [,127] [,128] [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138] [,139] [,140]
[1,]      5      6      1      5      6      1      5      5      4      1      4      1      5      5
     [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148] [,149] [,150] [,151] [,152] [,153] [,154]
[1,]      1      2      6      3      5      3      3      5      4      1      6      2      1      6
     [,155] [,156] [,157] [,158] [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]
[1,]      1      1      1      5      3      5      1      4      4      6      3      4      3      1
     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178] [,179] [,180] [,181] [,182]
[1,]      1      4      1      2      1      1      6      2      3      4      1      1      3      5
     [,183] [,184] [,185] [,186] [,187] [,188] [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196]
[1,]      4      6      5      4      4      6      1      1      3      6      4      4      3      5
     [,197] [,198] [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208] [,209] [,210]
[1,]      1      4      1      1      2      1      1      1      5      4      1      6      1      5
     [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218] [,219] [,220] [,221] [,222] [,223] [,224]
[1,]      1      1      6      1      2      6      2      4      4      2      4      1      4      6
     [,225] [,226] [,227] [,228] [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]
[1,]      6      6      6      2      6      3      5      6      5      6      4      6      6      1
     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248] [,249] [,250] [,251] [,252]
[1,]      6      6      6      5      2      4      5      2      2      4      3      3      5      6
     [,253] [,254] [,255] [,256] [,257] [,258] [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266]
[1,]      6      1      6      6      3      4      3      3      3      5      6      1      1      3
     [,267] [,268] [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278] [,279] [,280]
[1,]      6      4      6      4      6      6      2      3      1      6      5      4      2      2
     [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288] [,289] [,290] [,291] [,292] [,293] [,294]
[1,]      4      5      4      3      5      5      5      3      4      4      1      4      3      2
     [,295] [,296] [,297] [,298] [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]
[1,]      2      5      6      2      4      1      3      1      6      3      2      3      4      2
     [,309] [,310] [,311] [,312] [,313] [,314] [,315] [,316] [,317] [,318] [,319] [,320] [,321] [,322]
[1,]      5      3      5      6      6      3      2      3      4      2      2      6      3      4
     [,323] [,324] [,325] [,326] [,327] [,328] [,329] [,330] [,331] [,332] [,333] [,334] [,335] [,336]
[1,]      4      5      5      1      2      5      3      4      1      5      5      5      4      3
     [,337] [,338] [,339] [,340] [,341] [,342] [,343] [,344] [,345] [,346] [,347] [,348] [,349] [,350]
[1,]      5      4      1      4      3      4      3      4      2      6      4      2      4      3
     [,351] [,352] [,353] [,354] [,355] [,356] [,357] [,358] [,359] [,360] [,361] [,362] [,363] [,364]
[1,]      5      3      1      4      4      3      1      3      6      3      2      6      6      4
     [,365] [,366] [,367] [,368] [,369] [,370] [,371] [,372] [,373] [,374] [,375] [,376] [,377] [,378]
[1,]      4      1      5      1      6      5      6      4      1      1      3      6      6      2
     [,379] [,380] [,381] [,382] [,383] [,384] [,385] [,386] [,387] [,388] [,389] [,390] [,391] [,392]
[1,]      4      5      5      1      4      5      6      6      6      2      1      6      5      3
     [,393] [,394] [,395] [,396] [,397] [,398] [,399] [,400] [,401] [,402] [,403] [,404] [,405] [,406]
[1,]      2      4      5      4      5      6      5      4      1      1      4      2      5      4
     [,407] [,408] [,409] [,410] [,411] [,412] [,413] [,414] [,415] [,416] [,417] [,418] [,419] [,420]
[1,]      3      3      5      3      6      6      4      2      1      2      5      2      6      1
     [,421] [,422] [,423] [,424] [,425] [,426] [,427] [,428] [,429] [,430] [,431] [,432] [,433] [,434]
[1,]      1      3      6      4      6      2      4      5      1      1      2      3      2      2
     [,435] [,436] [,437] [,438] [,439] [,440] [,441] [,442] [,443] [,444] [,445] [,446] [,447] [,448]
[1,]      1      6      4      2      6      4      4      5      4      2      5      2      1      6
     [,449] [,450] [,451] [,452] [,453] [,454] [,455] [,456] [,457] [,458] [,459] [,460] [,461] [,462]
[1,]      3      1      2      6      1      5      3      5      3      4      6      6      4      4
     [,463] [,464] [,465] [,466] [,467] [,468] [,469] [,470] [,471] [,472] [,473] [,474] [,475] [,476]
[1,]      4      1      6      3      4      2      6      2      3      1      5      2      4      2
     [,477] [,478] [,479] [,480] [,481] [,482] [,483] [,484] [,485] [,486] [,487] [,488] [,489] [,490]
[1,]      2      3      4      3      2      6      4      4      4      6      3      3      5      5
     [,491] [,492] [,493] [,494] [,495] [,496] [,497] [,498] [,499] [,500] [,501] [,502] [,503] [,504]
[1,]      2      2      4      6      5      1      3      6      4      3      2      5      4      2
     [,505] [,506] [,507] [,508] [,509] [,510] [,511] [,512] [,513] [,514] [,515] [,516] [,517] [,518]
[1,]      5      3      6      6      2      6      4      3      3      4      3      4      6      2
     [,519] [,520] [,521] [,522] [,523] [,524] [,525] [,526] [,527] [,528] [,529] [,530] [,531] [,532]
[1,]      4      4      1      6      3      6      6      5      5      1      5      6      1      2
     [,533] [,534] [,535] [,536] [,537] [,538] [,539] [,540] [,541] [,542] [,543] [,544] [,545] [,546]
[1,]      5      1      2      2      3      3      1      3      2      1      3      2      2      4
     [,547] [,548] [,549] [,550] [,551] [,552] [,553] [,554] [,555] [,556] [,557] [,558] [,559] [,560]
[1,]      3      6      5      4      4      3      2      6      4      4      4      5      1      4
     [,561] [,562] [,563] [,564] [,565] [,566] [,567] [,568] [,569] [,570] [,571] [,572] [,573] [,574]
[1,]      1      3      5      5      4      6      5      3      2      1      2      1      1      4
     [,575] [,576] [,577] [,578] [,579] [,580] [,581] [,582] [,583] [,584] [,585] [,586] [,587] [,588]
[1,]      4      6      4      1      2      6      6      1      2      1      5      1      5      3
     [,589] [,590] [,591] [,592] [,593] [,594] [,595] [,596] [,597] [,598] [,599] [,600] [,601] [,602]
[1,]      1      5      6      5      4      3      1      3      6      3      1      2      1      2
     [,603] [,604] [,605] [,606] [,607] [,608] [,609] [,610] [,611] [,612] [,613] [,614] [,615] [,616]
[1,]      3      5      2      5      1      4      2      5      2      2      6      1      3      5
     [,617] [,618] [,619] [,620] [,621] [,622] [,623] [,624] [,625] [,626] [,627] [,628] [,629] [,630]
[1,]      1      4      2      4      5      6      2      6      4      2      5      1      1      6
     [,631] [,632] [,633] [,634] [,635] [,636] [,637] [,638] [,639] [,640] [,641] [,642] [,643] [,644]
[1,]      3      2      5      5      5      6      2      1      1      3      5      6      1      5
     [,645] [,646] [,647] [,648] [,649] [,650] [,651] [,652] [,653] [,654] [,655] [,656] [,657] [,658]
[1,]      5      1      4      4      6      1      5      6      4      6      2      4      6      5
     [,659] [,660] [,661] [,662] [,663] [,664] [,665] [,666] [,667] [,668] [,669] [,670] [,671] [,672]
[1,]      2      1      6      4      5      4      5      1      2      3      5      1      1      2
     [,673] [,674] [,675] [,676] [,677] [,678] [,679] [,680] [,681] [,682] [,683] [,684] [,685] [,686]
[1,]      5      5      5      6      6      3      2      2      5      3      1      2      2      3
     [,687] [,688] [,689] [,690] [,691] [,692] [,693] [,694] [,695] [,696] [,697] [,698] [,699] [,700]
[1,]      4      6      2      2      2      5      1      6      5      4      6      3      3      2
     [,701] [,702] [,703] [,704] [,705] [,706] [,707] [,708] [,709] [,710] [,711] [,712] [,713] [,714]
[1,]      1      5      3      5      1      3      1      3      4      1      4      3      6      3
     [,715] [,716] [,717] [,718] [,719] [,720] [,721] [,722] [,723] [,724] [,725] [,726] [,727] [,728]
[1,]      3      2      4      1      4      5      4      6      3      5      4      5      6      2
     [,729] [,730] [,731] [,732] [,733] [,734] [,735] [,736] [,737] [,738] [,739] [,740] [,741] [,742]
[1,]      2      5      5      1      2      3      2      4      3      4      2      2      5      2
     [,743] [,744] [,745] [,746] [,747] [,748] [,749] [,750] [,751] [,752] [,753] [,754] [,755] [,756]
[1,]      5      1      2      2      1      3      3      4      2      4      4      5      3      3
     [,757] [,758] [,759] [,760] [,761] [,762] [,763] [,764] [,765] [,766] [,767] [,768] [,769] [,770]
[1,]      4      5      6      4      4      3      1      6      5      2      5      5      5      1
     [,771] [,772] [,773] [,774] [,775] [,776] [,777] [,778] [,779] [,780] [,781] [,782] [,783] [,784]
[1,]      5      1      6      2      2      2      3      3      3      1      3      4      3      1
     [,785] [,786] [,787] [,788] [,789] [,790] [,791] [,792] [,793] [,794] [,795] [,796] [,797] [,798]
[1,]      6      3      2      3      2      5      6      4      4      6      4      3      3      3
     [,799] [,800] [,801] [,802] [,803] [,804] [,805] [,806] [,807] [,808] [,809] [,810] [,811] [,812]
[1,]      6      6      1      6      3      2      3      3      6      2      2      1      6      3
     [,813] [,814] [,815] [,816] [,817] [,818] [,819] [,820] [,821] [,822] [,823] [,824] [,825] [,826]
[1,]      4      4      2      3      3      3      5      6      3      5      2      2      5      1
     [,827] [,828] [,829] [,830] [,831] [,832] [,833] [,834] [,835] [,836] [,837] [,838] [,839] [,840]
[1,]      5      1      3      1      5      6      4      3      4      6      4      6      2      3
     [,841] [,842] [,843] [,844] [,845] [,846] [,847] [,848] [,849] [,850] [,851] [,852] [,853] [,854]
[1,]      1      3      5      6      3      6      1      2      6      3      6      4      4      1
     [,855] [,856] [,857] [,858] [,859] [,860] [,861] [,862] [,863] [,864] [,865] [,866] [,867] [,868]
[1,]      3      3      6      6      1      1      1      1      1      6      3      6      5      5
     [,869] [,870] [,871] [,872] [,873] [,874] [,875] [,876] [,877] [,878] [,879] [,880] [,881] [,882]
[1,]      2      5      3      6      4      6      4      2      5      3      1      4      6      1
     [,883] [,884] [,885] [,886] [,887] [,888] [,889] [,890] [,891] [,892] [,893] [,894] [,895] [,896]
[1,]      5      3      4      5      1      2      6      1      1      6      6      6      5      6
     [,897] [,898] [,899] [,900] [,901] [,902] [,903] [,904] [,905] [,906] [,907] [,908] [,909] [,910]
[1,]      6      4      3      5      1      1      2      3      6      2      6      5      5      2
     [,911] [,912] [,913] [,914] [,915] [,916] [,917] [,918] [,919] [,920] [,921] [,922] [,923] [,924]
[1,]      1      3      2      4      6      3      4      6      2      2      1      2      2      1
     [,925] [,926] [,927] [,928] [,929] [,930] [,931] [,932] [,933] [,934] [,935] [,936] [,937] [,938]
[1,]      3      3      6      4      4      4      3      5      6      5      2      3      2      6
     [,939] [,940] [,941] [,942] [,943] [,944] [,945] [,946] [,947] [,948] [,949] [,950] [,951] [,952]
[1,]      5      4      5      2      5      4      5      6      5      4      6      3      1      6
     [,953] [,954] [,955] [,956] [,957] [,958] [,959] [,960] [,961] [,962] [,963] [,964] [,965] [,966]
[1,]      1      2      5      1      1      1      1      4      4      4      2      2      2      1
     [,967] [,968] [,969] [,970] [,971] [,972] [,973] [,974] [,975] [,976] [,977] [,978] [,979] [,980]
[1,]      6      1      3      3      2      6      5      6      1      1      4      4      6      1
     [,981] [,982] [,983] [,984] [,985] [,986] [,987] [,988] [,989] [,990] [,991] [,992] [,993] [,994]
[1,]      1      6      5      1      3      5      6      6      4      6      6      2      4      1
     [,995] [,996] [,997] [,998] [,999] [,1000]
[1,]      6      1      3      1      2       4
 [ reached 'max' / getOption("max.print") -- omitted 5 rows and 9000 columns ]
txt> head(x)
txt> set.seed(1)
txt> set.seed(1)
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> x <- replicate(10000, sample(1:6, replace = TRUE))
txt> z <- (mean(x == 6) - p)/sqrt(p * (1 - p)/10000)
txt> z <- (mean(x == 6) - p)/sqrt(p * (1 - p)/10000)
txt> z
[1] -0.008944272
txt> z

 ## cannot answer last part of q because i didn't do sampling size correctly: we roll dice n times (n = 100), and perform the *simulation 10,000* times 

txt> txtComment("## cannot answer last part of q because i didn't do sampling size correctly: we roll dice n times (n = 100), and perform the *simulation 10,000* times")
txt> set.seed(1)
txt> set.seed(1)
txt> x <- sample(1:6, 100, replace = TRUE)
txt> x <- sample(1:6, 100, replace = TRUE)

 ## then get the mean of each sample, repeated 10000 times 

txt> txtComment("## then get the mean of each sample, repeated 10000 times")
txt> replicate(10000, mean(x == 6))
   [1] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
  [20] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
  [39] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
  [58] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
  [77] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
  [96] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [115] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [134] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [153] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [172] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [191] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [210] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [229] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [248] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [267] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [286] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [305] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [324] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [343] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [362] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [381] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [400] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [419] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [438] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [457] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [476] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [495] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [514] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [533] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [552] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [571] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [590] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [609] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [628] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [647] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [666] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [685] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [704] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [723] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [742] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [761] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [780] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [799] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [818] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [837] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [856] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [875] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [894] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [913] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [932] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [951] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [970] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [989] 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21 0.21
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> replicate(10000, mean(x == 6))
txt> Ns <- 1:6
txt> Ns <- 1:6
txt> Ns
[1] 1 2 3 4 5 6
txt> Ns
txt> set.seed(1)
txt> set.seed(1)
txt> res <- sapply(Ns, function(n) {
txt+ replicate(10000, mean(sample(1:6, 100, replace = TRUE)))
txt+ })
txt> res <- sapply(Ns, function(n) {
txt+ replicate(10000, mean(sample(1:6, 100, replace = TRUE)))
txt+ })
txt> head(res)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] 3.52 3.57 3.35 3.36 3.48 3.54
[2,] 3.33 3.35 3.55 3.69 3.48 3.39
[3,] 3.64 3.58 3.43 3.55 3.60 3.41
[4,] 3.34 3.70 3.33 3.47 3.80 3.36
[5,] 3.63 3.34 3.35 3.63 3.62 3.36
[6,] 3.61 3.23 3.49 3.51 3.68 3.68
txt> head(res)

 ## still can't calculate z score because no integer values to select x == 6 

txt> txtComment("## still can't calculate z score because no integer values to select x == 6")

 ## need to integrate z formula into function(n) 

txt> txtComment("## need to integrate z formula into function(n)")

 ## need to replicate the following 10,000x in {sub} curly brackets: create sample(1:6), roll dice 100x --> then calculate the mean value of each of those 100 die rolls (mean==6) --> then use that to calculate z score  

txt> txtComment("## need to replicate the following 10,000x in {sub
txt+ } curly brackets: create sample(1:6), roll dice 100x --> then calculate the mean value of each of those 100 die rolls (mean==6) --> then use that to calculate z score ")
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> head(res)
[1] 0.49574187 0.96049987 0.03098387 0.49574187 0.21688707 0.49574187
txt> head(res)

 ## now we have 10,000 z scores 

txt> txtComment("## now we have 10,000 z scores")

 ## in the replicate() simulation:
replicate(10000, { ... will open a curly brackets to tell R to repeat the following 10,000 times
x <- sample(1:6, 100, replace = TRUE)... will define x as the data frame of 100 different values, all ranging from integers 1-6 (with hopefully 1/6 of them being 6s)
a <- mean(x==6)... this pulls the average number of 6s (to confirm if we hopefully have about 1/6 of them as 6s). this will also be used for the z score formula next
z <- (a-p)... i.e. see how far the observed proportion is from the expected proportion (1/6)
/ sqrt(p*p(1-p)/n)... divide the above by the standard error to get the standardised z score, of mean = 0 and SD 1
 

txt> txtComment("## in the replicate() simulation:\nreplicate(10000, { ... will open a curly brackets to tell R to repeat the following 10,000 times\nx <- sample(1:6, 100, replace = TRUE)... will define x as the data frame of 100 different values, all ranging from integers 1-6 (with hopefully 1/6 of them being 6s)\na <- mean(x==6)... this pulls the average number of 6s (to confirm if we hopefully have about 1/6 of them as 6s). this will also be used for the z score formula next\nz <- (a-p)... i.e. see how far the observed proportion is from the expected proportion (1/6)\n/ sqrt(p*p(1-p)/n)... divide the above by the standard error to get the standardised z score, of mean = 0 and SD 1\n")

 ** and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).** 

txt> txtComment("** and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).**")

 ## the above res variable gave a value of 10,000 different z scores. use it to filter the values that res > 2 

txt> txtComment("## the above res variable gave a value of 10,000 different z scores. use it to filter the values that res > 2")
txt> res > 2
   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [17] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [33] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [65] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [81] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
  [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [113] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [161] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [209] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [225] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [257] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [305] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [321] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [353] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [369] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [401] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [417] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [449] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [465] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [497] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [513] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [545] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [593] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [609] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [641] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [657] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [689] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [705] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [737] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [753] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [785] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [833] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [849] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [881] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [897] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [929] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [945] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [977] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [993] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> res > 2
txt> sum(res > 2)
[1] 0
txt> sum(res > 2)
txt> head(res)
[1] 0.49574187 0.96049987 0.03098387 0.49574187 0.21688707 0.49574187
txt> head(res)
txt> mean(res > 2)
[1] 0
txt> mean(res > 2)
txt> mean(abs(res > 2))
[1] 0
txt> mean(abs(res > 2))
txt> set.seed(1)
txt> set.seed(1)
txt> replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
   [1]  0.40279027 -0.34082253  0.12393547 -0.52672574  0.12393547  0.03098387 -0.15491933  0.40279027
   [9] -0.06196773 -0.24787093  0.21688707 -0.24787093 -0.06196773 -0.06196773  0.30983867  0.03098387
  [17]  0.30983867 -0.06196773 -0.34082253 -0.52672574 -0.15491933 -0.43377413  0.40279027 -0.06196773
  [25] -0.06196773 -0.06196773 -0.43377413 -0.15491933  0.30983867  0.68164507  0.68164507  0.68164507
  [33] -0.24787093 -0.34082253  0.40279027  0.21688707  0.96049987  0.03098387  0.21688707 -0.61967734
  [41] -0.06196773  0.30983867  0.12393547  0.03098387 -0.06196773 -0.06196773 -0.06196773 -0.52672574
  [49] -0.15491933 -0.15491933 -0.06196773  0.21688707 -0.15491933 -0.06196773  0.03098387 -0.06196773
  [57]  0.40279027  0.21688707 -0.34082253  0.77459667  0.12393547  0.12393547  0.21688707  0.58869347
  [65] -0.24787093 -0.15491933  0.49574187 -0.71262894  0.03098387  0.49574187 -0.24787093 -0.43377413
  [73] -0.43377413 -0.15491933 -0.24787093  0.03098387 -0.15491933  0.12393547 -0.34082253 -0.06196773
  [81] -0.52672574 -0.52672574 -0.34082253 -0.52672574 -0.06196773  0.21688707  0.21688707  0.77459667
  [89] -0.34082253 -0.34082253 -0.15491933  0.58869347  0.49574187 -0.43377413  0.03098387 -0.15491933
  [97] -0.06196773 -0.15491933  0.03098387 -0.15491933  0.30983867  0.03098387 -0.15491933 -0.34082253
 [105] -0.15491933  0.12393547 -0.06196773 -0.71262894 -1.08443534 -0.15491933  0.30983867 -0.52672574
 [113]  0.21688707 -0.06196773  0.03098387  0.03098387 -0.52672574  0.68164507  0.58869347 -0.34082253
 [121]  0.12393547 -0.15491933 -0.06196773  0.03098387 -0.34082253 -0.71262894 -0.43377413 -0.34082253
 [129] -0.43377413 -0.06196773  0.58869347  0.03098387 -0.52672574  0.12393547 -0.34082253 -0.15491933
 [137]  0.21688707  0.12393547  0.21688707  0.86754827 -0.34082253 -0.24787093  0.12393547  0.30983867
 [145] -0.15491933 -0.61967734 -0.24787093  0.49574187  0.03098387  0.40279027 -0.34082253 -0.52672574
 [153]  0.49574187 -0.52672574 -0.06196773  0.21688707 -0.61967734  0.12393547 -0.06196773  0.21688707
 [161]  0.12393547 -0.52672574 -0.24787093 -0.43377413  0.30983867 -0.80558054  0.03098387 -0.80558054
 [169]  0.40279027  0.49574187 -0.24787093  0.12393547  0.21688707 -0.43377413  0.12393547 -0.15491933
 [177]  0.68164507  0.77459667 -0.06196773 -0.61967734 -0.52672574  0.49574187  0.30983867  0.03098387
 [185]  0.58869347  0.21688707 -0.24787093  0.12393547 -0.34082253 -0.71262894  0.40279027  0.03098387
 [193]  0.30983867  0.21688707 -0.24787093 -0.34082253 -0.15491933 -0.06196773 -0.15491933  0.12393547
 [201] -0.34082253  0.03098387 -0.24787093  0.12393547  0.03098387 -0.15491933  0.03098387  0.49574187
 [209] -0.34082253 -0.15491933 -0.06196773  0.03098387  0.21688707  0.58869347  0.49574187  0.68164507
 [217] -0.15491933 -0.15491933  0.03098387 -0.15491933 -0.06196773  0.40279027 -0.06196773  0.21688707
 [225] -0.52672574 -0.34082253 -0.34082253 -0.06196773 -0.15491933 -0.06196773  0.30983867  0.68164507
 [233] -0.06196773 -0.52672574  0.30983867  0.21688707 -0.15491933  0.03098387  0.03098387 -0.52672574
 [241]  0.03098387 -0.43377413 -0.06196773 -0.06196773 -0.15491933 -0.24787093 -0.52672574  0.03098387
 [249]  0.03098387  0.40279027 -0.34082253  0.12393547 -0.61967734  0.40279027 -0.15491933  0.03098387
 [257]  0.21688707  0.03098387  0.21688707  0.40279027  0.03098387 -0.52672574 -0.24787093 -0.52672574
 [265]  0.12393547  0.30983867  0.03098387 -0.06196773  0.12393547 -0.06196773  0.58869347  0.12393547
 [273]  0.12393547  0.58869347 -0.34082253  0.12393547  0.03098387  0.21688707 -0.71262894  0.58869347
 [281] -0.43377413 -0.15491933  0.49574187 -0.61967734  0.12393547  0.12393547 -0.15491933  0.12393547
 [289] -0.52672574  0.40279027 -0.80558054 -0.24787093 -0.24787093  0.49574187 -0.15491933  0.12393547
 [297] -0.06196773  0.68164507 -0.15491933 -0.61967734  0.21688707  1.51820947 -0.43377413  0.30983867
 [305]  0.30983867  0.03098387  0.40279027  0.03098387 -0.34082253 -0.15491933 -0.24787093 -0.15491933
 [313]  0.03098387  0.03098387 -0.34082253  0.03098387 -0.43377413 -0.52672574  0.03098387 -0.43377413
 [321] -0.15491933 -0.06196773  0.40279027  0.03098387 -0.61967734  0.49574187  0.77459667 -0.34082253
 [329] -0.06196773 -0.15491933 -0.15491933 -0.34082253 -0.15491933  0.03098387  0.30983867 -0.43377413
 [337]  0.12393547  0.40279027 -0.15491933  0.12393547  1.05345147 -0.34082253 -0.24787093 -0.15491933
 [345] -0.24787093  0.12393547 -0.24787093  0.03098387 -0.34082253 -0.06196773 -0.24787093  0.21688707
 [353] -0.52672574  0.49574187 -0.24787093 -0.24787093  0.21688707 -0.15491933 -0.71262894  0.40279027
 [361] -0.34082253 -0.24787093  0.03098387  0.40279027 -0.34082253  0.21688707 -0.34082253  0.40279027
 [369] -0.15491933 -0.15491933  0.21688707  0.58869347 -0.24787093  0.68164507  0.12393547  0.86754827
 [377] -0.24787093  0.30983867  0.12393547  0.40279027  0.40279027  0.58869347  0.21688707  0.03098387
 [385] -0.06196773  0.21688707 -0.24787093  0.03098387 -0.24787093  0.58869347 -0.24787093  0.30983867
 [393]  0.30983867  0.21688707  0.40279027  1.14640307 -0.06196773 -0.43377413  0.03098387 -0.06196773
 [401]  0.58869347  0.49574187 -0.34082253  0.40279027 -0.06196773 -0.43377413  0.21688707 -0.61967734
 [409] -0.15491933  0.58869347 -0.15491933  0.12393547 -0.06196773 -0.52672574  0.86754827 -0.06196773
 [417]  0.68164507 -0.06196773 -0.06196773 -0.06196773 -0.34082253 -0.34082253 -0.15491933 -0.52672574
 [425] -0.06196773  0.12393547  0.21688707  0.03098387  0.12393547 -0.61967734 -0.43377413 -0.15491933
 [433]  0.03098387 -0.43377413  0.40279027 -0.15491933 -0.06196773  0.49574187  0.03098387  0.30983867
 [441]  0.40279027  0.58869347 -0.24787093  0.12393547 -0.61967734 -0.43377413  0.03098387  0.03098387
 [449]  0.03098387 -0.06196773  0.96049987 -0.15491933  0.49574187  0.12393547 -0.34082253  0.40279027
 [457] -0.15491933  0.40279027  0.58869347  0.21688707 -0.06196773 -0.24787093  0.12393547  0.77459667
 [465]  0.12393547  0.12393547  0.12393547  0.77459667  0.49574187  0.21688707  0.49574187 -0.15491933
 [473] -0.71262894  0.58869347  0.49574187 -0.71262894  0.21688707  0.03098387  0.49574187 -0.24787093
 [481] -0.06196773  0.49574187  0.21688707  0.12393547  0.12393547  0.03098387  0.30983867  0.30983867
 [489] -0.06196773 -0.15491933  0.12393547  0.30983867  0.30983867  0.68164507  0.12393547 -0.06196773
 [497]  0.03098387  0.12393547  0.03098387 -0.34082253 -0.24787093 -0.71262894 -0.15491933 -0.06196773
 [505] -0.06196773  0.49574187  0.12393547  0.30983867  0.40279027 -0.61967734  0.21688707 -0.15491933
 [513] -0.24787093 -0.24787093  0.12393547 -0.34082253  0.21688707  0.03098387 -0.24787093 -0.15491933
 [521]  0.12393547 -0.34082253 -0.34082253 -0.43377413  0.21688707  0.03098387 -0.52672574  0.03098387
 [529] -0.43377413  0.49574187  0.40279027 -0.15491933 -0.15491933 -0.24787093  0.40279027 -0.24787093
 [537]  0.03098387  0.03098387 -0.34082253  0.03098387  0.12393547  0.12393547 -0.24787093 -0.06196773
 [545]  0.40279027  0.03098387 -0.24787093  0.21688707  0.40279027  0.58869347 -0.24787093  0.58869347
 [553] -0.34082253  0.21688707 -0.52672574  0.21688707 -0.15491933  0.68164507  0.40279027  0.21688707
 [561] -0.34082253  0.21688707 -0.80558054 -0.24787093 -0.24787093 -0.15491933 -0.06196773 -0.99148374
 [569]  0.03098387  0.40279027 -0.15491933 -0.15491933 -0.24787093 -0.24787093 -0.43377413 -0.06196773
 [577] -0.24787093  0.12393547  0.12393547 -0.71262894  0.40279027  0.21688707  0.49574187  0.12393547
 [585]  0.49574187  0.12393547 -0.06196773 -0.15491933 -0.43377413 -0.34082253 -0.34082253 -0.34082253
 [593] -0.15491933 -0.15491933  0.21688707 -0.06196773 -0.61967734  0.30983867  0.12393547  0.12393547
 [601]  0.12393547  0.03098387  0.03098387 -0.06196773  0.21688707 -0.71262894 -0.52672574 -0.15491933
 [609]  0.03098387  0.30983867  0.21688707 -0.06196773  0.03098387 -0.52672574  0.12393547  0.68164507
 [617] -0.43377413  0.12393547  0.40279027 -0.06196773  0.12393547 -0.24787093 -0.06196773  0.21688707
 [625] -0.52672574 -0.43377413 -0.52672574  0.21688707  0.40279027 -0.06196773  0.03098387 -0.06196773
 [633] -0.06196773  0.03098387  0.77459667  0.40279027  0.49574187 -0.06196773 -0.43377413 -0.24787093
 [641]  0.21688707  0.12393547  0.03098387  0.21688707 -0.61967734 -0.52672574 -0.43377413 -0.15491933
 [649]  0.30983867 -0.24787093  0.68164507  0.12393547  0.21688707 -0.24787093 -0.24787093 -0.06196773
 [657]  0.12393547 -0.06196773  0.30983867  0.12393547 -0.34082253 -0.06196773  0.30983867  0.68164507
 [665]  0.03098387 -0.61967734 -0.15491933 -0.24787093 -0.15491933  0.58869347  0.03098387  0.12393547
 [673] -0.06196773 -0.15491933 -0.06196773  0.21688707 -0.15491933  0.03098387 -0.71262894  0.40279027
 [681] -0.34082253  0.03098387  0.03098387  0.21688707  0.21688707  0.12393547  0.12393547  0.68164507
 [689] -0.43377413 -0.34082253 -0.52672574 -0.06196773  0.58869347  0.12393547  0.12393547 -0.24787093
 [697]  0.40279027  0.21688707 -0.06196773 -0.43377413 -0.24787093 -0.06196773  0.12393547  0.03098387
 [705] -0.06196773  0.21688707 -0.24787093  0.03098387 -0.24787093 -0.06196773 -0.06196773 -0.34082253
 [713] -0.34082253  0.30983867  0.12393547  0.03098387  0.03098387 -0.06196773 -0.15491933  0.49574187
 [721] -0.34082253  0.12393547 -0.52672574 -0.15491933  0.21688707 -0.06196773 -0.52672574  0.03098387
 [729]  0.21688707 -0.06196773  0.21688707 -0.71262894 -0.34082253 -0.15491933 -0.34082253 -0.34082253
 [737] -0.15491933 -0.06196773 -0.06196773 -0.06196773  0.30983867  0.03098387 -0.34082253 -0.61967734
 [745]  0.30983867 -0.24787093  0.21688707  0.40279027 -0.24787093  0.12393547  0.30983867  0.12393547
 [753]  0.12393547 -0.06196773  0.03098387  0.30983867  0.03098387  0.12393547  0.49574187 -0.24787093
 [761] -0.15491933 -0.15491933  0.12393547  0.21688707 -0.43377413  0.40279027  0.40279027 -0.06196773
 [769] -0.24787093  0.68164507  0.77459667 -0.06196773 -0.24787093  0.12393547  0.49574187 -0.15491933
 [777]  0.03098387 -0.15491933  0.03098387 -0.34082253  0.12393547  0.03098387  0.77459667  0.12393547
 [785]  0.21688707  0.03098387 -0.15491933 -0.34082253  0.12393547  0.40279027  0.21688707  0.30983867
 [793] -0.61967734  0.30983867  0.58869347 -0.15491933  0.40279027  0.12393547 -0.24787093 -0.71262894
 [801]  0.40279027 -0.06196773  0.21688707 -0.24787093  0.12393547 -0.15491933  0.12393547 -0.24787093
 [809] -0.24787093 -0.06196773  0.49574187  0.03098387  0.30983867 -0.34082253  0.21688707 -0.43377413
 [817] -0.06196773 -0.52672574  0.03098387  0.30983867  0.30983867  0.21688707 -0.52672574  0.49574187
 [825]  0.03098387  0.03098387  0.21688707  0.40279027 -0.15491933  0.58869347 -0.24787093  0.49574187
 [833]  0.03098387 -0.24787093 -0.43377413  0.58869347 -0.34082253 -0.15491933 -0.15491933 -0.24787093
 [841] -0.24787093  0.77459667  0.03098387  0.12393547 -0.06196773 -0.15491933  0.03098387 -0.34082253
 [849]  0.21688707 -0.52672574 -0.06196773  0.12393547  0.21688707  0.49574187  0.30983867  0.12393547
 [857] -0.52672574  0.03098387 -0.15491933 -0.15491933 -0.34082253 -0.15491933 -0.15491933  0.77459667
 [865]  1.14640307 -0.06196773 -0.15491933 -0.43377413  0.21688707 -0.24787093 -0.43377413  0.30983867
 [873] -0.06196773  0.30983867  0.40279027 -0.34082253  0.21688707 -0.06196773 -0.06196773 -0.34082253
 [881]  0.21688707  0.30983867 -0.15491933 -0.06196773 -0.06196773  0.21688707  0.21688707 -0.34082253
 [889]  0.21688707  0.12393547  0.49574187  0.68164507  0.21688707  0.21688707  0.49574187  0.49574187
 [897] -0.24787093  0.30983867 -0.61967734  0.77459667  0.21688707  0.49574187  0.68164507  0.49574187
 [905] -0.15491933  0.12393547  0.12393547 -0.34082253 -0.15491933  0.12393547  0.12393547  0.40279027
 [913]  0.03098387  0.40279027  0.30983867 -0.34082253  0.12393547  0.12393547  0.21688707  0.68164507
 [921] -0.61967734  0.21688707 -0.43377413  0.21688707 -0.43377413 -0.15491933 -0.71262894  0.03098387
 [929] -0.52672574  0.03098387 -0.89853214  0.12393547  0.68164507 -0.15491933  0.40279027 -0.34082253
 [937] -0.15491933  0.03098387  0.03098387 -0.06196773  0.21688707  0.12393547 -0.43377413 -0.06196773
 [945]  0.30983867 -0.06196773 -0.06196773  0.21688707 -0.15491933 -0.06196773  0.30983867 -0.15491933
 [953] -0.24787093  0.12393547  0.03098387  0.49574187 -0.24787093 -0.06196773  0.03098387  0.03098387
 [961]  0.40279027 -0.24787093 -0.15491933  0.12393547  0.40279027  0.40279027  0.30983867 -0.24787093
 [969] -0.24787093 -0.24787093  0.03098387 -0.34082253 -0.15491933 -0.43377413 -0.06196773 -0.06196773
 [977]  0.49574187  0.12393547 -0.43377413 -0.15491933  0.40279027  0.21688707  0.58869347 -0.24787093
 [985]  0.40279027  0.21688707 -0.24787093  0.40279027 -0.61967734  0.12393547  0.30983867  0.03098387
 [993] -0.61967734  0.21688707 -0.15491933 -0.24787093 -0.52672574  0.12393547 -0.06196773  0.21688707
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> set.seed(1)
txt> set.seed(1)
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, 100, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> head(res)
[1]  0.40279027 -0.34082253  0.12393547 -0.52672574  0.12393547  0.03098387
txt> head(res)
txt> mean(res > 2)
[1] 0
txt> mean(res > 2)
txt> abs(res)
   [1] 0.40279027 0.34082253 0.12393547 0.52672574 0.12393547 0.03098387 0.15491933 0.40279027
   [9] 0.06196773 0.24787093 0.21688707 0.24787093 0.06196773 0.06196773 0.30983867 0.03098387
  [17] 0.30983867 0.06196773 0.34082253 0.52672574 0.15491933 0.43377413 0.40279027 0.06196773
  [25] 0.06196773 0.06196773 0.43377413 0.15491933 0.30983867 0.68164507 0.68164507 0.68164507
  [33] 0.24787093 0.34082253 0.40279027 0.21688707 0.96049987 0.03098387 0.21688707 0.61967734
  [41] 0.06196773 0.30983867 0.12393547 0.03098387 0.06196773 0.06196773 0.06196773 0.52672574
  [49] 0.15491933 0.15491933 0.06196773 0.21688707 0.15491933 0.06196773 0.03098387 0.06196773
  [57] 0.40279027 0.21688707 0.34082253 0.77459667 0.12393547 0.12393547 0.21688707 0.58869347
  [65] 0.24787093 0.15491933 0.49574187 0.71262894 0.03098387 0.49574187 0.24787093 0.43377413
  [73] 0.43377413 0.15491933 0.24787093 0.03098387 0.15491933 0.12393547 0.34082253 0.06196773
  [81] 0.52672574 0.52672574 0.34082253 0.52672574 0.06196773 0.21688707 0.21688707 0.77459667
  [89] 0.34082253 0.34082253 0.15491933 0.58869347 0.49574187 0.43377413 0.03098387 0.15491933
  [97] 0.06196773 0.15491933 0.03098387 0.15491933 0.30983867 0.03098387 0.15491933 0.34082253
 [105] 0.15491933 0.12393547 0.06196773 0.71262894 1.08443534 0.15491933 0.30983867 0.52672574
 [113] 0.21688707 0.06196773 0.03098387 0.03098387 0.52672574 0.68164507 0.58869347 0.34082253
 [121] 0.12393547 0.15491933 0.06196773 0.03098387 0.34082253 0.71262894 0.43377413 0.34082253
 [129] 0.43377413 0.06196773 0.58869347 0.03098387 0.52672574 0.12393547 0.34082253 0.15491933
 [137] 0.21688707 0.12393547 0.21688707 0.86754827 0.34082253 0.24787093 0.12393547 0.30983867
 [145] 0.15491933 0.61967734 0.24787093 0.49574187 0.03098387 0.40279027 0.34082253 0.52672574
 [153] 0.49574187 0.52672574 0.06196773 0.21688707 0.61967734 0.12393547 0.06196773 0.21688707
 [161] 0.12393547 0.52672574 0.24787093 0.43377413 0.30983867 0.80558054 0.03098387 0.80558054
 [169] 0.40279027 0.49574187 0.24787093 0.12393547 0.21688707 0.43377413 0.12393547 0.15491933
 [177] 0.68164507 0.77459667 0.06196773 0.61967734 0.52672574 0.49574187 0.30983867 0.03098387
 [185] 0.58869347 0.21688707 0.24787093 0.12393547 0.34082253 0.71262894 0.40279027 0.03098387
 [193] 0.30983867 0.21688707 0.24787093 0.34082253 0.15491933 0.06196773 0.15491933 0.12393547
 [201] 0.34082253 0.03098387 0.24787093 0.12393547 0.03098387 0.15491933 0.03098387 0.49574187
 [209] 0.34082253 0.15491933 0.06196773 0.03098387 0.21688707 0.58869347 0.49574187 0.68164507
 [217] 0.15491933 0.15491933 0.03098387 0.15491933 0.06196773 0.40279027 0.06196773 0.21688707
 [225] 0.52672574 0.34082253 0.34082253 0.06196773 0.15491933 0.06196773 0.30983867 0.68164507
 [233] 0.06196773 0.52672574 0.30983867 0.21688707 0.15491933 0.03098387 0.03098387 0.52672574
 [241] 0.03098387 0.43377413 0.06196773 0.06196773 0.15491933 0.24787093 0.52672574 0.03098387
 [249] 0.03098387 0.40279027 0.34082253 0.12393547 0.61967734 0.40279027 0.15491933 0.03098387
 [257] 0.21688707 0.03098387 0.21688707 0.40279027 0.03098387 0.52672574 0.24787093 0.52672574
 [265] 0.12393547 0.30983867 0.03098387 0.06196773 0.12393547 0.06196773 0.58869347 0.12393547
 [273] 0.12393547 0.58869347 0.34082253 0.12393547 0.03098387 0.21688707 0.71262894 0.58869347
 [281] 0.43377413 0.15491933 0.49574187 0.61967734 0.12393547 0.12393547 0.15491933 0.12393547
 [289] 0.52672574 0.40279027 0.80558054 0.24787093 0.24787093 0.49574187 0.15491933 0.12393547
 [297] 0.06196773 0.68164507 0.15491933 0.61967734 0.21688707 1.51820947 0.43377413 0.30983867
 [305] 0.30983867 0.03098387 0.40279027 0.03098387 0.34082253 0.15491933 0.24787093 0.15491933
 [313] 0.03098387 0.03098387 0.34082253 0.03098387 0.43377413 0.52672574 0.03098387 0.43377413
 [321] 0.15491933 0.06196773 0.40279027 0.03098387 0.61967734 0.49574187 0.77459667 0.34082253
 [329] 0.06196773 0.15491933 0.15491933 0.34082253 0.15491933 0.03098387 0.30983867 0.43377413
 [337] 0.12393547 0.40279027 0.15491933 0.12393547 1.05345147 0.34082253 0.24787093 0.15491933
 [345] 0.24787093 0.12393547 0.24787093 0.03098387 0.34082253 0.06196773 0.24787093 0.21688707
 [353] 0.52672574 0.49574187 0.24787093 0.24787093 0.21688707 0.15491933 0.71262894 0.40279027
 [361] 0.34082253 0.24787093 0.03098387 0.40279027 0.34082253 0.21688707 0.34082253 0.40279027
 [369] 0.15491933 0.15491933 0.21688707 0.58869347 0.24787093 0.68164507 0.12393547 0.86754827
 [377] 0.24787093 0.30983867 0.12393547 0.40279027 0.40279027 0.58869347 0.21688707 0.03098387
 [385] 0.06196773 0.21688707 0.24787093 0.03098387 0.24787093 0.58869347 0.24787093 0.30983867
 [393] 0.30983867 0.21688707 0.40279027 1.14640307 0.06196773 0.43377413 0.03098387 0.06196773
 [401] 0.58869347 0.49574187 0.34082253 0.40279027 0.06196773 0.43377413 0.21688707 0.61967734
 [409] 0.15491933 0.58869347 0.15491933 0.12393547 0.06196773 0.52672574 0.86754827 0.06196773
 [417] 0.68164507 0.06196773 0.06196773 0.06196773 0.34082253 0.34082253 0.15491933 0.52672574
 [425] 0.06196773 0.12393547 0.21688707 0.03098387 0.12393547 0.61967734 0.43377413 0.15491933
 [433] 0.03098387 0.43377413 0.40279027 0.15491933 0.06196773 0.49574187 0.03098387 0.30983867
 [441] 0.40279027 0.58869347 0.24787093 0.12393547 0.61967734 0.43377413 0.03098387 0.03098387
 [449] 0.03098387 0.06196773 0.96049987 0.15491933 0.49574187 0.12393547 0.34082253 0.40279027
 [457] 0.15491933 0.40279027 0.58869347 0.21688707 0.06196773 0.24787093 0.12393547 0.77459667
 [465] 0.12393547 0.12393547 0.12393547 0.77459667 0.49574187 0.21688707 0.49574187 0.15491933
 [473] 0.71262894 0.58869347 0.49574187 0.71262894 0.21688707 0.03098387 0.49574187 0.24787093
 [481] 0.06196773 0.49574187 0.21688707 0.12393547 0.12393547 0.03098387 0.30983867 0.30983867
 [489] 0.06196773 0.15491933 0.12393547 0.30983867 0.30983867 0.68164507 0.12393547 0.06196773
 [497] 0.03098387 0.12393547 0.03098387 0.34082253 0.24787093 0.71262894 0.15491933 0.06196773
 [505] 0.06196773 0.49574187 0.12393547 0.30983867 0.40279027 0.61967734 0.21688707 0.15491933
 [513] 0.24787093 0.24787093 0.12393547 0.34082253 0.21688707 0.03098387 0.24787093 0.15491933
 [521] 0.12393547 0.34082253 0.34082253 0.43377413 0.21688707 0.03098387 0.52672574 0.03098387
 [529] 0.43377413 0.49574187 0.40279027 0.15491933 0.15491933 0.24787093 0.40279027 0.24787093
 [537] 0.03098387 0.03098387 0.34082253 0.03098387 0.12393547 0.12393547 0.24787093 0.06196773
 [545] 0.40279027 0.03098387 0.24787093 0.21688707 0.40279027 0.58869347 0.24787093 0.58869347
 [553] 0.34082253 0.21688707 0.52672574 0.21688707 0.15491933 0.68164507 0.40279027 0.21688707
 [561] 0.34082253 0.21688707 0.80558054 0.24787093 0.24787093 0.15491933 0.06196773 0.99148374
 [569] 0.03098387 0.40279027 0.15491933 0.15491933 0.24787093 0.24787093 0.43377413 0.06196773
 [577] 0.24787093 0.12393547 0.12393547 0.71262894 0.40279027 0.21688707 0.49574187 0.12393547
 [585] 0.49574187 0.12393547 0.06196773 0.15491933 0.43377413 0.34082253 0.34082253 0.34082253
 [593] 0.15491933 0.15491933 0.21688707 0.06196773 0.61967734 0.30983867 0.12393547 0.12393547
 [601] 0.12393547 0.03098387 0.03098387 0.06196773 0.21688707 0.71262894 0.52672574 0.15491933
 [609] 0.03098387 0.30983867 0.21688707 0.06196773 0.03098387 0.52672574 0.12393547 0.68164507
 [617] 0.43377413 0.12393547 0.40279027 0.06196773 0.12393547 0.24787093 0.06196773 0.21688707
 [625] 0.52672574 0.43377413 0.52672574 0.21688707 0.40279027 0.06196773 0.03098387 0.06196773
 [633] 0.06196773 0.03098387 0.77459667 0.40279027 0.49574187 0.06196773 0.43377413 0.24787093
 [641] 0.21688707 0.12393547 0.03098387 0.21688707 0.61967734 0.52672574 0.43377413 0.15491933
 [649] 0.30983867 0.24787093 0.68164507 0.12393547 0.21688707 0.24787093 0.24787093 0.06196773
 [657] 0.12393547 0.06196773 0.30983867 0.12393547 0.34082253 0.06196773 0.30983867 0.68164507
 [665] 0.03098387 0.61967734 0.15491933 0.24787093 0.15491933 0.58869347 0.03098387 0.12393547
 [673] 0.06196773 0.15491933 0.06196773 0.21688707 0.15491933 0.03098387 0.71262894 0.40279027
 [681] 0.34082253 0.03098387 0.03098387 0.21688707 0.21688707 0.12393547 0.12393547 0.68164507
 [689] 0.43377413 0.34082253 0.52672574 0.06196773 0.58869347 0.12393547 0.12393547 0.24787093
 [697] 0.40279027 0.21688707 0.06196773 0.43377413 0.24787093 0.06196773 0.12393547 0.03098387
 [705] 0.06196773 0.21688707 0.24787093 0.03098387 0.24787093 0.06196773 0.06196773 0.34082253
 [713] 0.34082253 0.30983867 0.12393547 0.03098387 0.03098387 0.06196773 0.15491933 0.49574187
 [721] 0.34082253 0.12393547 0.52672574 0.15491933 0.21688707 0.06196773 0.52672574 0.03098387
 [729] 0.21688707 0.06196773 0.21688707 0.71262894 0.34082253 0.15491933 0.34082253 0.34082253
 [737] 0.15491933 0.06196773 0.06196773 0.06196773 0.30983867 0.03098387 0.34082253 0.61967734
 [745] 0.30983867 0.24787093 0.21688707 0.40279027 0.24787093 0.12393547 0.30983867 0.12393547
 [753] 0.12393547 0.06196773 0.03098387 0.30983867 0.03098387 0.12393547 0.49574187 0.24787093
 [761] 0.15491933 0.15491933 0.12393547 0.21688707 0.43377413 0.40279027 0.40279027 0.06196773
 [769] 0.24787093 0.68164507 0.77459667 0.06196773 0.24787093 0.12393547 0.49574187 0.15491933
 [777] 0.03098387 0.15491933 0.03098387 0.34082253 0.12393547 0.03098387 0.77459667 0.12393547
 [785] 0.21688707 0.03098387 0.15491933 0.34082253 0.12393547 0.40279027 0.21688707 0.30983867
 [793] 0.61967734 0.30983867 0.58869347 0.15491933 0.40279027 0.12393547 0.24787093 0.71262894
 [801] 0.40279027 0.06196773 0.21688707 0.24787093 0.12393547 0.15491933 0.12393547 0.24787093
 [809] 0.24787093 0.06196773 0.49574187 0.03098387 0.30983867 0.34082253 0.21688707 0.43377413
 [817] 0.06196773 0.52672574 0.03098387 0.30983867 0.30983867 0.21688707 0.52672574 0.49574187
 [825] 0.03098387 0.03098387 0.21688707 0.40279027 0.15491933 0.58869347 0.24787093 0.49574187
 [833] 0.03098387 0.24787093 0.43377413 0.58869347 0.34082253 0.15491933 0.15491933 0.24787093
 [841] 0.24787093 0.77459667 0.03098387 0.12393547 0.06196773 0.15491933 0.03098387 0.34082253
 [849] 0.21688707 0.52672574 0.06196773 0.12393547 0.21688707 0.49574187 0.30983867 0.12393547
 [857] 0.52672574 0.03098387 0.15491933 0.15491933 0.34082253 0.15491933 0.15491933 0.77459667
 [865] 1.14640307 0.06196773 0.15491933 0.43377413 0.21688707 0.24787093 0.43377413 0.30983867
 [873] 0.06196773 0.30983867 0.40279027 0.34082253 0.21688707 0.06196773 0.06196773 0.34082253
 [881] 0.21688707 0.30983867 0.15491933 0.06196773 0.06196773 0.21688707 0.21688707 0.34082253
 [889] 0.21688707 0.12393547 0.49574187 0.68164507 0.21688707 0.21688707 0.49574187 0.49574187
 [897] 0.24787093 0.30983867 0.61967734 0.77459667 0.21688707 0.49574187 0.68164507 0.49574187
 [905] 0.15491933 0.12393547 0.12393547 0.34082253 0.15491933 0.12393547 0.12393547 0.40279027
 [913] 0.03098387 0.40279027 0.30983867 0.34082253 0.12393547 0.12393547 0.21688707 0.68164507
 [921] 0.61967734 0.21688707 0.43377413 0.21688707 0.43377413 0.15491933 0.71262894 0.03098387
 [929] 0.52672574 0.03098387 0.89853214 0.12393547 0.68164507 0.15491933 0.40279027 0.34082253
 [937] 0.15491933 0.03098387 0.03098387 0.06196773 0.21688707 0.12393547 0.43377413 0.06196773
 [945] 0.30983867 0.06196773 0.06196773 0.21688707 0.15491933 0.06196773 0.30983867 0.15491933
 [953] 0.24787093 0.12393547 0.03098387 0.49574187 0.24787093 0.06196773 0.03098387 0.03098387
 [961] 0.40279027 0.24787093 0.15491933 0.12393547 0.40279027 0.40279027 0.30983867 0.24787093
 [969] 0.24787093 0.24787093 0.03098387 0.34082253 0.15491933 0.43377413 0.06196773 0.06196773
 [977] 0.49574187 0.12393547 0.43377413 0.15491933 0.40279027 0.21688707 0.58869347 0.24787093
 [985] 0.40279027 0.21688707 0.24787093 0.40279027 0.61967734 0.12393547 0.30983867 0.03098387
 [993] 0.61967734 0.21688707 0.15491933 0.24787093 0.52672574 0.12393547 0.06196773 0.21688707
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> abs(res)
txt> mean(abs(res > 2))
[1] 0
txt> mean(abs(res > 2))
txt> summary(res)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-1.084435 -0.247871 -0.061968 -0.005026  0.216887  1.518209 
txt> summary(res)
txt> qqnorm(res)
txt> qqnorm(res)
txt> res <- abs(res)
txt> res <- abs(res)
txt> head(res)
[1] 0.40279027 0.34082253 0.12393547 0.52672574 0.12393547 0.03098387
txt> head(res)
txt> qqnorm(res)
txt> qqnorm(res)
txt> n = 100
txt> n = 100
txt> set.seed(1)
txt> set.seed(1)
txt> replicate(10000, {
txt+ x <- sample(1:6, n, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
   [1]  1.16275535 -0.98386991  0.35777088 -1.52052622  0.35777088  0.08944272 -0.44721360  1.16275535
   [9] -0.17888544 -0.71554175  0.62609903 -0.71554175 -0.17888544 -0.17888544  0.89442719  0.08944272
  [17]  0.89442719 -0.17888544 -0.98386991 -1.52052622 -0.44721360 -1.25219807  1.16275535 -0.17888544
  [25] -0.17888544 -0.17888544 -1.25219807 -0.44721360  0.89442719  1.96773982  1.96773982  1.96773982
  [33] -0.71554175 -0.98386991  1.16275535  0.62609903  2.77272429  0.08944272  0.62609903 -1.78885438
  [41] -0.17888544  0.89442719  0.35777088  0.08944272 -0.17888544 -0.17888544 -0.17888544 -1.52052622
  [49] -0.44721360 -0.44721360 -0.17888544  0.62609903 -0.44721360 -0.17888544  0.08944272 -0.17888544
  [57]  1.16275535  0.62609903 -0.98386991  2.23606798  0.35777088  0.35777088  0.62609903  1.69941166
  [65] -0.71554175 -0.44721360  1.43108351 -2.05718254  0.08944272  1.43108351 -0.71554175 -1.25219807
  [73] -1.25219807 -0.44721360 -0.71554175  0.08944272 -0.44721360  0.35777088 -0.98386991 -0.17888544
  [81] -1.52052622 -1.52052622 -0.98386991 -1.52052622 -0.17888544  0.62609903  0.62609903  2.23606798
  [89] -0.98386991 -0.98386991 -0.44721360  1.69941166  1.43108351 -1.25219807  0.08944272 -0.44721360
  [97] -0.17888544 -0.44721360  0.08944272 -0.44721360  0.89442719  0.08944272 -0.44721360 -0.98386991
 [105] -0.44721360  0.35777088 -0.17888544 -2.05718254 -3.13049517 -0.44721360  0.89442719 -1.52052622
 [113]  0.62609903 -0.17888544  0.08944272  0.08944272 -1.52052622  1.96773982  1.69941166 -0.98386991
 [121]  0.35777088 -0.44721360 -0.17888544  0.08944272 -0.98386991 -2.05718254 -1.25219807 -0.98386991
 [129] -1.25219807 -0.17888544  1.69941166  0.08944272 -1.52052622  0.35777088 -0.98386991 -0.44721360
 [137]  0.62609903  0.35777088  0.62609903  2.50439613 -0.98386991 -0.71554175  0.35777088  0.89442719
 [145] -0.44721360 -1.78885438 -0.71554175  1.43108351  0.08944272  1.16275535 -0.98386991 -1.52052622
 [153]  1.43108351 -1.52052622 -0.17888544  0.62609903 -1.78885438  0.35777088 -0.17888544  0.62609903
 [161]  0.35777088 -1.52052622 -0.71554175 -1.25219807  0.89442719 -2.32551070  0.08944272 -2.32551070
 [169]  1.16275535  1.43108351 -0.71554175  0.35777088  0.62609903 -1.25219807  0.35777088 -0.44721360
 [177]  1.96773982  2.23606798 -0.17888544 -1.78885438 -1.52052622  1.43108351  0.89442719  0.08944272
 [185]  1.69941166  0.62609903 -0.71554175  0.35777088 -0.98386991 -2.05718254  1.16275535  0.08944272
 [193]  0.89442719  0.62609903 -0.71554175 -0.98386991 -0.44721360 -0.17888544 -0.44721360  0.35777088
 [201] -0.98386991  0.08944272 -0.71554175  0.35777088  0.08944272 -0.44721360  0.08944272  1.43108351
 [209] -0.98386991 -0.44721360 -0.17888544  0.08944272  0.62609903  1.69941166  1.43108351  1.96773982
 [217] -0.44721360 -0.44721360  0.08944272 -0.44721360 -0.17888544  1.16275535 -0.17888544  0.62609903
 [225] -1.52052622 -0.98386991 -0.98386991 -0.17888544 -0.44721360 -0.17888544  0.89442719  1.96773982
 [233] -0.17888544 -1.52052622  0.89442719  0.62609903 -0.44721360  0.08944272  0.08944272 -1.52052622
 [241]  0.08944272 -1.25219807 -0.17888544 -0.17888544 -0.44721360 -0.71554175 -1.52052622  0.08944272
 [249]  0.08944272  1.16275535 -0.98386991  0.35777088 -1.78885438  1.16275535 -0.44721360  0.08944272
 [257]  0.62609903  0.08944272  0.62609903  1.16275535  0.08944272 -1.52052622 -0.71554175 -1.52052622
 [265]  0.35777088  0.89442719  0.08944272 -0.17888544  0.35777088 -0.17888544  1.69941166  0.35777088
 [273]  0.35777088  1.69941166 -0.98386991  0.35777088  0.08944272  0.62609903 -2.05718254  1.69941166
 [281] -1.25219807 -0.44721360  1.43108351 -1.78885438  0.35777088  0.35777088 -0.44721360  0.35777088
 [289] -1.52052622  1.16275535 -2.32551070 -0.71554175 -0.71554175  1.43108351 -0.44721360  0.35777088
 [297] -0.17888544  1.96773982 -0.44721360 -1.78885438  0.62609903  4.38269324 -1.25219807  0.89442719
 [305]  0.89442719  0.08944272  1.16275535  0.08944272 -0.98386991 -0.44721360 -0.71554175 -0.44721360
 [313]  0.08944272  0.08944272 -0.98386991  0.08944272 -1.25219807 -1.52052622  0.08944272 -1.25219807
 [321] -0.44721360 -0.17888544  1.16275535  0.08944272 -1.78885438  1.43108351  2.23606798 -0.98386991
 [329] -0.17888544 -0.44721360 -0.44721360 -0.98386991 -0.44721360  0.08944272  0.89442719 -1.25219807
 [337]  0.35777088  1.16275535 -0.44721360  0.35777088  3.04105245 -0.98386991 -0.71554175 -0.44721360
 [345] -0.71554175  0.35777088 -0.71554175  0.08944272 -0.98386991 -0.17888544 -0.71554175  0.62609903
 [353] -1.52052622  1.43108351 -0.71554175 -0.71554175  0.62609903 -0.44721360 -2.05718254  1.16275535
 [361] -0.98386991 -0.71554175  0.08944272  1.16275535 -0.98386991  0.62609903 -0.98386991  1.16275535
 [369] -0.44721360 -0.44721360  0.62609903  1.69941166 -0.71554175  1.96773982  0.35777088  2.50439613
 [377] -0.71554175  0.89442719  0.35777088  1.16275535  1.16275535  1.69941166  0.62609903  0.08944272
 [385] -0.17888544  0.62609903 -0.71554175  0.08944272 -0.71554175  1.69941166 -0.71554175  0.89442719
 [393]  0.89442719  0.62609903  1.16275535  3.30938061 -0.17888544 -1.25219807  0.08944272 -0.17888544
 [401]  1.69941166  1.43108351 -0.98386991  1.16275535 -0.17888544 -1.25219807  0.62609903 -1.78885438
 [409] -0.44721360  1.69941166 -0.44721360  0.35777088 -0.17888544 -1.52052622  2.50439613 -0.17888544
 [417]  1.96773982 -0.17888544 -0.17888544 -0.17888544 -0.98386991 -0.98386991 -0.44721360 -1.52052622
 [425] -0.17888544  0.35777088  0.62609903  0.08944272  0.35777088 -1.78885438 -1.25219807 -0.44721360
 [433]  0.08944272 -1.25219807  1.16275535 -0.44721360 -0.17888544  1.43108351  0.08944272  0.89442719
 [441]  1.16275535  1.69941166 -0.71554175  0.35777088 -1.78885438 -1.25219807  0.08944272  0.08944272
 [449]  0.08944272 -0.17888544  2.77272429 -0.44721360  1.43108351  0.35777088 -0.98386991  1.16275535
 [457] -0.44721360  1.16275535  1.69941166  0.62609903 -0.17888544 -0.71554175  0.35777088  2.23606798
 [465]  0.35777088  0.35777088  0.35777088  2.23606798  1.43108351  0.62609903  1.43108351 -0.44721360
 [473] -2.05718254  1.69941166  1.43108351 -2.05718254  0.62609903  0.08944272  1.43108351 -0.71554175
 [481] -0.17888544  1.43108351  0.62609903  0.35777088  0.35777088  0.08944272  0.89442719  0.89442719
 [489] -0.17888544 -0.44721360  0.35777088  0.89442719  0.89442719  1.96773982  0.35777088 -0.17888544
 [497]  0.08944272  0.35777088  0.08944272 -0.98386991 -0.71554175 -2.05718254 -0.44721360 -0.17888544
 [505] -0.17888544  1.43108351  0.35777088  0.89442719  1.16275535 -1.78885438  0.62609903 -0.44721360
 [513] -0.71554175 -0.71554175  0.35777088 -0.98386991  0.62609903  0.08944272 -0.71554175 -0.44721360
 [521]  0.35777088 -0.98386991 -0.98386991 -1.25219807  0.62609903  0.08944272 -1.52052622  0.08944272
 [529] -1.25219807  1.43108351  1.16275535 -0.44721360 -0.44721360 -0.71554175  1.16275535 -0.71554175
 [537]  0.08944272  0.08944272 -0.98386991  0.08944272  0.35777088  0.35777088 -0.71554175 -0.17888544
 [545]  1.16275535  0.08944272 -0.71554175  0.62609903  1.16275535  1.69941166 -0.71554175  1.69941166
 [553] -0.98386991  0.62609903 -1.52052622  0.62609903 -0.44721360  1.96773982  1.16275535  0.62609903
 [561] -0.98386991  0.62609903 -2.32551070 -0.71554175 -0.71554175 -0.44721360 -0.17888544 -2.86216701
 [569]  0.08944272  1.16275535 -0.44721360 -0.44721360 -0.71554175 -0.71554175 -1.25219807 -0.17888544
 [577] -0.71554175  0.35777088  0.35777088 -2.05718254  1.16275535  0.62609903  1.43108351  0.35777088
 [585]  1.43108351  0.35777088 -0.17888544 -0.44721360 -1.25219807 -0.98386991 -0.98386991 -0.98386991
 [593] -0.44721360 -0.44721360  0.62609903 -0.17888544 -1.78885438  0.89442719  0.35777088  0.35777088
 [601]  0.35777088  0.08944272  0.08944272 -0.17888544  0.62609903 -2.05718254 -1.52052622 -0.44721360
 [609]  0.08944272  0.89442719  0.62609903 -0.17888544  0.08944272 -1.52052622  0.35777088  1.96773982
 [617] -1.25219807  0.35777088  1.16275535 -0.17888544  0.35777088 -0.71554175 -0.17888544  0.62609903
 [625] -1.52052622 -1.25219807 -1.52052622  0.62609903  1.16275535 -0.17888544  0.08944272 -0.17888544
 [633] -0.17888544  0.08944272  2.23606798  1.16275535  1.43108351 -0.17888544 -1.25219807 -0.71554175
 [641]  0.62609903  0.35777088  0.08944272  0.62609903 -1.78885438 -1.52052622 -1.25219807 -0.44721360
 [649]  0.89442719 -0.71554175  1.96773982  0.35777088  0.62609903 -0.71554175 -0.71554175 -0.17888544
 [657]  0.35777088 -0.17888544  0.89442719  0.35777088 -0.98386991 -0.17888544  0.89442719  1.96773982
 [665]  0.08944272 -1.78885438 -0.44721360 -0.71554175 -0.44721360  1.69941166  0.08944272  0.35777088
 [673] -0.17888544 -0.44721360 -0.17888544  0.62609903 -0.44721360  0.08944272 -2.05718254  1.16275535
 [681] -0.98386991  0.08944272  0.08944272  0.62609903  0.62609903  0.35777088  0.35777088  1.96773982
 [689] -1.25219807 -0.98386991 -1.52052622 -0.17888544  1.69941166  0.35777088  0.35777088 -0.71554175
 [697]  1.16275535  0.62609903 -0.17888544 -1.25219807 -0.71554175 -0.17888544  0.35777088  0.08944272
 [705] -0.17888544  0.62609903 -0.71554175  0.08944272 -0.71554175 -0.17888544 -0.17888544 -0.98386991
 [713] -0.98386991  0.89442719  0.35777088  0.08944272  0.08944272 -0.17888544 -0.44721360  1.43108351
 [721] -0.98386991  0.35777088 -1.52052622 -0.44721360  0.62609903 -0.17888544 -1.52052622  0.08944272
 [729]  0.62609903 -0.17888544  0.62609903 -2.05718254 -0.98386991 -0.44721360 -0.98386991 -0.98386991
 [737] -0.44721360 -0.17888544 -0.17888544 -0.17888544  0.89442719  0.08944272 -0.98386991 -1.78885438
 [745]  0.89442719 -0.71554175  0.62609903  1.16275535 -0.71554175  0.35777088  0.89442719  0.35777088
 [753]  0.35777088 -0.17888544  0.08944272  0.89442719  0.08944272  0.35777088  1.43108351 -0.71554175
 [761] -0.44721360 -0.44721360  0.35777088  0.62609903 -1.25219807  1.16275535  1.16275535 -0.17888544
 [769] -0.71554175  1.96773982  2.23606798 -0.17888544 -0.71554175  0.35777088  1.43108351 -0.44721360
 [777]  0.08944272 -0.44721360  0.08944272 -0.98386991  0.35777088  0.08944272  2.23606798  0.35777088
 [785]  0.62609903  0.08944272 -0.44721360 -0.98386991  0.35777088  1.16275535  0.62609903  0.89442719
 [793] -1.78885438  0.89442719  1.69941166 -0.44721360  1.16275535  0.35777088 -0.71554175 -2.05718254
 [801]  1.16275535 -0.17888544  0.62609903 -0.71554175  0.35777088 -0.44721360  0.35777088 -0.71554175
 [809] -0.71554175 -0.17888544  1.43108351  0.08944272  0.89442719 -0.98386991  0.62609903 -1.25219807
 [817] -0.17888544 -1.52052622  0.08944272  0.89442719  0.89442719  0.62609903 -1.52052622  1.43108351
 [825]  0.08944272  0.08944272  0.62609903  1.16275535 -0.44721360  1.69941166 -0.71554175  1.43108351
 [833]  0.08944272 -0.71554175 -1.25219807  1.69941166 -0.98386991 -0.44721360 -0.44721360 -0.71554175
 [841] -0.71554175  2.23606798  0.08944272  0.35777088 -0.17888544 -0.44721360  0.08944272 -0.98386991
 [849]  0.62609903 -1.52052622 -0.17888544  0.35777088  0.62609903  1.43108351  0.89442719  0.35777088
 [857] -1.52052622  0.08944272 -0.44721360 -0.44721360 -0.98386991 -0.44721360 -0.44721360  2.23606798
 [865]  3.30938061 -0.17888544 -0.44721360 -1.25219807  0.62609903 -0.71554175 -1.25219807  0.89442719
 [873] -0.17888544  0.89442719  1.16275535 -0.98386991  0.62609903 -0.17888544 -0.17888544 -0.98386991
 [881]  0.62609903  0.89442719 -0.44721360 -0.17888544 -0.17888544  0.62609903  0.62609903 -0.98386991
 [889]  0.62609903  0.35777088  1.43108351  1.96773982  0.62609903  0.62609903  1.43108351  1.43108351
 [897] -0.71554175  0.89442719 -1.78885438  2.23606798  0.62609903  1.43108351  1.96773982  1.43108351
 [905] -0.44721360  0.35777088  0.35777088 -0.98386991 -0.44721360  0.35777088  0.35777088  1.16275535
 [913]  0.08944272  1.16275535  0.89442719 -0.98386991  0.35777088  0.35777088  0.62609903  1.96773982
 [921] -1.78885438  0.62609903 -1.25219807  0.62609903 -1.25219807 -0.44721360 -2.05718254  0.08944272
 [929] -1.52052622  0.08944272 -2.59383885  0.35777088  1.96773982 -0.44721360  1.16275535 -0.98386991
 [937] -0.44721360  0.08944272  0.08944272 -0.17888544  0.62609903  0.35777088 -1.25219807 -0.17888544
 [945]  0.89442719 -0.17888544 -0.17888544  0.62609903 -0.44721360 -0.17888544  0.89442719 -0.44721360
 [953] -0.71554175  0.35777088  0.08944272  1.43108351 -0.71554175 -0.17888544  0.08944272  0.08944272
 [961]  1.16275535 -0.71554175 -0.44721360  0.35777088  1.16275535  1.16275535  0.89442719 -0.71554175
 [969] -0.71554175 -0.71554175  0.08944272 -0.98386991 -0.44721360 -1.25219807 -0.17888544 -0.17888544
 [977]  1.43108351  0.35777088 -1.25219807 -0.44721360  1.16275535  0.62609903  1.69941166 -0.71554175
 [985]  1.16275535  0.62609903 -0.71554175  1.16275535 -1.78885438  0.35777088  0.89442719  0.08944272
 [993] -1.78885438  0.62609903 -0.44721360 -0.71554175 -1.52052622  0.35777088 -0.17888544  0.62609903
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> replicate(10000, {
txt+ x <- sample(1:6, n, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> set.seed(1)
txt> set.seed(1)
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, n, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> res <- replicate(10000, {
txt+ x <- sample(1:6, n, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - p)/sqrt(p * (1 - p)/n)
txt+ })
txt> head(res)
[1]  1.16275535 -0.98386991  0.35777088 -1.52052622  0.35777088  0.08944272
txt> head(res)
txt> mean(abs(res > 2))
[1] 0.0219
txt> mean(abs(res > 2))

 ## qqnorm was looking weird, and couldn't get a proportion of z > 2 because of mistake - didn't define n as 100. now 0.02 looks closer to the CLT expected of 0.05 in the q 

txt> txtComment("## qqnorm was looking weird, and couldn't get a proportion of z > 2 because of mistake - didn't define n as 100. now 0.02 looks closer to the CLT expected of 0.05 in the q")
txt> qqnorm(res)
txt> qqnorm(res)
txt> mean(abs(res) > 2)
[1] 0.0431
txt> mean(abs(res) > 2)

 ## don't define res > 2 in abs()! 

txt> txtComment("## don't define res > 2 in abs()!")

 ## get the abs() value of res dataset first, then compare that to filter which of those abs(res) values are > 2 

txt> txtComment("## get the abs() value of res dataset first, then compare that to filter which of those abs(res) values are > 2")

 ## the reason why the value doubled from 0.02 to 0.04 was because I was only looking at one end of the tail (the positive end), because the abs() value couldnt correctly change the -2 values to positive to be taken into account in the res > 2 filter. 

txt> txtComment("## the reason why the value doubled from 0.02 to 0.04 was because I was only looking at one end of the tail (the positive end), because the abs() value couldnt correctly change the -2 values to positive to be taken into account in the res > 2 filter.")

txt> `?`(txt)
No documentation for ‘txt’ in specified packages and libraries:
you could try ‘??txt’

 **In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to “kick in”.

Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best?

A) p=0.5 and n=5
B) p=0.5 and n=30
C) p=0.01 and n=30
D) p=0.01 and n=100
 


 ## would expect lower probability to have lower variance in the data, vice versa. i.e. if p = 0.05, you would have 50/50 TRUE FALSE, but p = 0.05, you would have more TRUE than FALSE (vice versa), hence less variation and central tendency to one of the two. 


 ## a higher n sample size would mean a larger divisble factor in the z score formula, hence a smaller value for SD. this means that a larger n means less variance in the data (silences noise). CLT also 'kicks in' with a larger sample size i.e. achieves closer to a total normal distribution (asymptotic) 


 ##  D seems to be the most likely answer for the least variance, and most normal distribution? 

txt> res <- replicate(10000, {
txt+ x <- sample(1:6, 30, replace = TRUE)
txt+ a <- mean(x == 6)
txt+ z <- (a - 0.01)/sqrt(0.01 * (1 - 0.01)/30)
txt+ })
txt> res
   [1]  4.9543369  6.7892766 10.4591558  8.6242162  6.7892766  4.9543369  8.6242162 12.2940954  6.7892766
  [10]  6.7892766 17.7989142  8.6242162 14.1290350 15.9639746  6.7892766  8.6242162  4.9543369  3.1193973
  [19]  4.9543369  4.9543369  3.1193973 10.4591558 10.4591558  4.9543369  8.6242162  6.7892766 12.2940954
  [28]  8.6242162 10.4591558  6.7892766  8.6242162  6.7892766  8.6242162 15.9639746 15.9639746 12.2940954
  [37]  8.6242162 10.4591558 14.1290350  8.6242162 10.4591558 14.1290350 10.4591558 10.4591558  8.6242162
  [46]  6.7892766  3.1193973  6.7892766  8.6242162  8.6242162  4.9543369  6.7892766  6.7892766 14.1290350
  [55]  1.2844577  4.9543369  8.6242162  6.7892766  4.9543369  3.1193973  4.9543369  6.7892766 10.4591558
  [64] 10.4591558 12.2940954  8.6242162  8.6242162 10.4591558 14.1290350 12.2940954 14.1290350 10.4591558
  [73]  6.7892766 15.9639746  8.6242162  6.7892766 10.4591558 10.4591558 12.2940954  8.6242162  3.1193973
  [82]  6.7892766  8.6242162 10.4591558 10.4591558 14.1290350  4.9543369  6.7892766 14.1290350  6.7892766
  [91]  8.6242162  4.9543369 10.4591558  6.7892766  8.6242162  8.6242162  4.9543369 12.2940954  6.7892766
 [100]  4.9543369  8.6242162  8.6242162 10.4591558  6.7892766  8.6242162  4.9543369 12.2940954 10.4591558
 [109] 10.4591558  8.6242162  8.6242162  3.1193973 14.1290350  1.2844577  1.2844577  6.7892766  3.1193973
 [118]  4.9543369  8.6242162  1.2844577 10.4591558  4.9543369  4.9543369  6.7892766 12.2940954  3.1193973
 [127] 17.7989142  8.6242162  4.9543369  6.7892766  3.1193973 12.2940954 12.2940954  8.6242162 10.4591558
 [136]  3.1193973 10.4591558  6.7892766 12.2940954 14.1290350  8.6242162  3.1193973 17.7989142 12.2940954
 [145]  8.6242162  4.9543369 12.2940954  3.1193973 -0.5504819  8.6242162  6.7892766  4.9543369  8.6242162
 [154] 12.2940954 10.4591558 14.1290350 15.9639746  1.2844577 14.1290350 15.9639746  4.9543369  8.6242162
 [163]  8.6242162  6.7892766 10.4591558  6.7892766  6.7892766  4.9543369  8.6242162  3.1193973 -0.5504819
 [172]  8.6242162  8.6242162  4.9543369 10.4591558 14.1290350  3.1193973 15.9639746  8.6242162  4.9543369
 [181]  6.7892766  6.7892766  4.9543369  6.7892766 12.2940954 14.1290350  3.1193973 10.4591558  6.7892766
 [190] 12.2940954  8.6242162 12.2940954  8.6242162  8.6242162 15.9639746  1.2844577  4.9543369  8.6242162
 [199]  8.6242162  4.9543369 10.4591558 10.4591558  6.7892766 10.4591558  4.9543369  4.9543369 14.1290350
 [208]  8.6242162 10.4591558  4.9543369  3.1193973 12.2940954  6.7892766  3.1193973  8.6242162  8.6242162
 [217] 10.4591558 10.4591558 10.4591558  6.7892766 10.4591558  8.6242162  8.6242162 10.4591558 10.4591558
 [226] 14.1290350  8.6242162  8.6242162  4.9543369 -0.5504819  8.6242162  1.2844577  4.9543369  8.6242162
 [235] 17.7989142  6.7892766  8.6242162  8.6242162 10.4591558 14.1290350  4.9543369  4.9543369  8.6242162
 [244]  3.1193973 10.4591558 10.4591558  8.6242162  8.6242162  6.7892766  3.1193973  8.6242162  6.7892766
 [253] 14.1290350  4.9543369  6.7892766  4.9543369  8.6242162  4.9543369 12.2940954 12.2940954 12.2940954
 [262] 10.4591558 14.1290350  4.9543369 12.2940954  4.9543369  6.7892766  8.6242162  8.6242162  1.2844577
 [271]  8.6242162 14.1290350  6.7892766  4.9543369  3.1193973  3.1193973  6.7892766  6.7892766  8.6242162
 [280] 10.4591558 10.4591558  8.6242162 14.1290350  3.1193973  8.6242162  6.7892766 10.4591558  8.6242162
 [289] 14.1290350 10.4591558 12.2940954 10.4591558  8.6242162 10.4591558 14.1290350  6.7892766  8.6242162
 [298] 12.2940954 12.2940954 -0.5504819 10.4591558  8.6242162  8.6242162  8.6242162 10.4591558 10.4591558
 [307] 15.9639746  8.6242162 12.2940954 12.2940954  3.1193973  3.1193973 10.4591558 10.4591558 10.4591558
 [316]  4.9543369  6.7892766 12.2940954 15.9639746 10.4591558  3.1193973  4.9543369 10.4591558 10.4591558
 [325] 14.1290350 12.2940954  6.7892766 10.4591558 15.9639746 10.4591558  8.6242162  8.6242162  3.1193973
 [334]  1.2844577  6.7892766  6.7892766 10.4591558  8.6242162 12.2940954  6.7892766  6.7892766  6.7892766
 [343]  4.9543369  1.2844577  4.9543369  8.6242162  3.1193973  8.6242162 10.4591558 10.4591558 12.2940954
 [352] 10.4591558  3.1193973  6.7892766 14.1290350  4.9543369  8.6242162 10.4591558  4.9543369  6.7892766
 [361] 10.4591558  6.7892766 14.1290350 10.4591558  8.6242162 10.4591558  6.7892766 12.2940954  8.6242162
 [370]  3.1193973 -0.5504819  6.7892766 12.2940954  6.7892766  6.7892766 12.2940954 14.1290350 12.2940954
 [379]  3.1193973  1.2844577 14.1290350  8.6242162  6.7892766  6.7892766  8.6242162  3.1193973 14.1290350
 [388]  8.6242162  1.2844577 15.9639746  4.9543369  8.6242162  8.6242162 10.4591558  4.9543369  8.6242162
 [397] 10.4591558 12.2940954  3.1193973  8.6242162  8.6242162  6.7892766  4.9543369  8.6242162  4.9543369
 [406]  3.1193973  6.7892766 10.4591558  3.1193973  8.6242162 10.4591558  1.2844577  8.6242162  4.9543369
 [415]  6.7892766 17.7989142  6.7892766  8.6242162 12.2940954 12.2940954 14.1290350  8.6242162  6.7892766
 [424]  6.7892766  3.1193973 10.4591558  6.7892766  8.6242162 17.7989142  8.6242162  3.1193973  4.9543369
 [433] 12.2940954  8.6242162 10.4591558 10.4591558  4.9543369  8.6242162 12.2940954  4.9543369 12.2940954
 [442]  6.7892766 19.6338538  4.9543369  1.2844577 19.6338538 14.1290350 10.4591558 12.2940954 14.1290350
 [451]  6.7892766  6.7892766 10.4591558 14.1290350  6.7892766  8.6242162  4.9543369 10.4591558  4.9543369
 [460]  6.7892766  8.6242162 10.4591558  8.6242162 10.4591558 12.2940954  8.6242162 14.1290350  8.6242162
 [469]  4.9543369 -0.5504819 10.4591558  6.7892766  8.6242162 10.4591558  4.9543369  4.9543369 12.2940954
 [478] 12.2940954  6.7892766  6.7892766 10.4591558  1.2844577 10.4591558 14.1290350  8.6242162 14.1290350
 [487]  3.1193973  6.7892766  6.7892766  4.9543369 14.1290350  6.7892766 10.4591558 10.4591558  8.6242162
 [496]  8.6242162  6.7892766 10.4591558 10.4591558  6.7892766 10.4591558  8.6242162  8.6242162  4.9543369
 [505]  4.9543369  4.9543369  6.7892766  4.9543369  8.6242162  3.1193973  8.6242162  6.7892766 10.4591558
 [514]  6.7892766 12.2940954  6.7892766 15.9639746  4.9543369  6.7892766  4.9543369  4.9543369  8.6242162
 [523]  4.9543369  8.6242162 10.4591558  1.2844577  6.7892766  8.6242162 15.9639746  4.9543369  6.7892766
 [532]  8.6242162  1.2844577  6.7892766  8.6242162  3.1193973 10.4591558 12.2940954  1.2844577  8.6242162
 [541]  4.9543369  6.7892766  3.1193973  6.7892766  6.7892766  8.6242162 12.2940954 15.9639746  8.6242162
 [550]  3.1193973 12.2940954  4.9543369  8.6242162 10.4591558  8.6242162  4.9543369  4.9543369 15.9639746
 [559]  6.7892766  6.7892766 14.1290350 14.1290350 12.2940954  8.6242162  8.6242162  6.7892766 10.4591558
 [568]  6.7892766  1.2844577 12.2940954  6.7892766 17.7989142  6.7892766 12.2940954  8.6242162  4.9543369
 [577]  6.7892766 15.9639746 14.1290350  3.1193973  6.7892766  8.6242162  8.6242162  8.6242162  6.7892766
 [586] 12.2940954 10.4591558  3.1193973  3.1193973  8.6242162  8.6242162  8.6242162  6.7892766  6.7892766
 [595] 10.4591558  4.9543369 10.4591558 21.4687934  4.9543369  1.2844577  6.7892766  4.9543369  6.7892766
 [604]  8.6242162 17.7989142  8.6242162  8.6242162  1.2844577  6.7892766 10.4591558 14.1290350  8.6242162
 [613]  6.7892766 10.4591558 12.2940954  4.9543369 25.1386726  8.6242162  1.2844577  8.6242162 10.4591558
 [622]  6.7892766 15.9639746 15.9639746 10.4591558 12.2940954 14.1290350  4.9543369  4.9543369  3.1193973
 [631]  8.6242162  1.2844577 10.4591558  8.6242162  6.7892766  8.6242162  4.9543369  4.9543369 10.4591558
 [640] 17.7989142  8.6242162  3.1193973 10.4591558  6.7892766  3.1193973  6.7892766  4.9543369  6.7892766
 [649]  8.6242162 10.4591558  4.9543369  6.7892766  6.7892766  8.6242162  4.9543369 14.1290350  4.9543369
 [658]  8.6242162  6.7892766  4.9543369  6.7892766 10.4591558  6.7892766 10.4591558  6.7892766 12.2940954
 [667]  8.6242162  6.7892766 12.2940954 12.2940954  6.7892766 10.4591558  8.6242162 12.2940954  4.9543369
 [676] 10.4591558  4.9543369 12.2940954  6.7892766  4.9543369  3.1193973  8.6242162  4.9543369 10.4591558
 [685] 10.4591558  4.9543369  8.6242162  4.9543369  6.7892766 19.6338538 10.4591558 15.9639746  6.7892766
 [694]  1.2844577  8.6242162 14.1290350 15.9639746 12.2940954  6.7892766  6.7892766 10.4591558 10.4591558
 [703] 10.4591558 10.4591558  6.7892766  8.6242162  4.9543369 15.9639746 15.9639746  6.7892766 12.2940954
 [712]  6.7892766  8.6242162 12.2940954  8.6242162  8.6242162  6.7892766  8.6242162 17.7989142 10.4591558
 [721]  6.7892766 14.1290350 12.2940954  4.9543369  3.1193973  6.7892766 10.4591558 12.2940954  3.1193973
 [730]  8.6242162  4.9543369  6.7892766 14.1290350  1.2844577 15.9639746  4.9543369  8.6242162  3.1193973
 [739]  4.9543369 12.2940954  6.7892766  3.1193973  4.9543369  4.9543369  8.6242162 10.4591558 14.1290350
 [748] 12.2940954  8.6242162  8.6242162 12.2940954  8.6242162 10.4591558  8.6242162  6.7892766 10.4591558
 [757]  6.7892766  8.6242162 12.2940954  8.6242162  6.7892766  4.9543369 15.9639746  6.7892766  8.6242162
 [766]  4.9543369 12.2940954  8.6242162  4.9543369 15.9639746 10.4591558  8.6242162  4.9543369  4.9543369
 [775] 12.2940954  6.7892766 10.4591558  6.7892766  6.7892766  6.7892766 10.4591558  6.7892766  1.2844577
 [784]  6.7892766 14.1290350 12.2940954 10.4591558 10.4591558  1.2844577 10.4591558 10.4591558  8.6242162
 [793] 10.4591558  4.9543369  6.7892766  8.6242162 12.2940954  6.7892766  6.7892766  8.6242162  8.6242162
 [802]  6.7892766 10.4591558  3.1193973  8.6242162  4.9543369  8.6242162  4.9543369  4.9543369 14.1290350
 [811]  4.9543369  3.1193973  8.6242162  4.9543369  8.6242162  3.1193973  8.6242162 14.1290350  8.6242162
 [820] 10.4591558  1.2844577  8.6242162  8.6242162 10.4591558 15.9639746  8.6242162  6.7892766 10.4591558
 [829]  8.6242162  4.9543369 14.1290350 14.1290350  8.6242162 12.2940954  4.9543369 14.1290350  6.7892766
 [838]  8.6242162  4.9543369 10.4591558 12.2940954  3.1193973  6.7892766 12.2940954  3.1193973  6.7892766
 [847] 10.4591558 12.2940954  4.9543369  1.2844577  8.6242162 15.9639746 10.4591558  8.6242162  3.1193973
 [856]  8.6242162 12.2940954 10.4591558  4.9543369  4.9543369  3.1193973 10.4591558 10.4591558 10.4591558
 [865] 14.1290350 12.2940954 14.1290350 12.2940954 14.1290350  6.7892766  8.6242162  4.9543369  8.6242162
 [874] 12.2940954  3.1193973  3.1193973 12.2940954 14.1290350  8.6242162  4.9543369  4.9543369  6.7892766
 [883]  3.1193973  6.7892766 14.1290350  3.1193973  8.6242162  8.6242162 10.4591558  3.1193973  8.6242162
 [892] 15.9639746  8.6242162 14.1290350 10.4591558 12.2940954 12.2940954  6.7892766 -0.5504819  4.9543369
 [901] 12.2940954  6.7892766  4.9543369  4.9543369  4.9543369  4.9543369  8.6242162 14.1290350 12.2940954
 [910]  4.9543369 15.9639746 10.4591558  8.6242162  4.9543369 10.4591558 10.4591558  8.6242162  4.9543369
 [919]  4.9543369  6.7892766  8.6242162 15.9639746  3.1193973 10.4591558 14.1290350  6.7892766  8.6242162
 [928]  6.7892766 15.9639746 14.1290350  8.6242162  6.7892766  1.2844577  6.7892766 14.1290350 14.1290350
 [937] -0.5504819 10.4591558  3.1193973  6.7892766 10.4591558 10.4591558  6.7892766  8.6242162 14.1290350
 [946]  8.6242162 10.4591558  6.7892766  6.7892766 10.4591558  4.9543369  6.7892766 14.1290350  1.2844577
 [955]  6.7892766 10.4591558  6.7892766 10.4591558 12.2940954 10.4591558  8.6242162 10.4591558 12.2940954
 [964]  4.9543369 10.4591558 12.2940954 10.4591558 14.1290350 12.2940954  8.6242162  6.7892766 -0.5504819
 [973] 12.2940954  4.9543369  4.9543369  6.7892766  6.7892766  8.6242162 14.1290350  3.1193973  6.7892766
 [982] 14.1290350  8.6242162 10.4591558 14.1290350  8.6242162 10.4591558 15.9639746 14.1290350  8.6242162
 [991]  4.9543369  1.2844577 10.4591558 10.4591558  3.1193973 12.2940954 10.4591558 10.4591558  8.6242162
[1000] 10.4591558
 [ reached 'max' / getOption("max.print") -- omitted 9000 entries ]
txt> mean(abs(res) > 2)
[1] 0.9682
txt> qqnorm(res)
txt> res2 <- replicate(10000, {
txt+ x <- sample(1:6, 30, replace = TRUE)
txt+ })
txt> res2 <- replicate(1000, {
txt+ a <- mean(x == 6)
txt+ x <- sample(1:6, 5, replace = TRUE)
txt+ })
txt> qqnorm(res2)
txt> qqnorm(res)

 ## res2 (p - 0.5, n = 35) had the longest 'tail' ends, i.e. large quantities of noise and variance. therefore, res1 (p = 0.01, n = 30) had the least deviating 'tail' ends, therefore closer to a normal distribution 


 ** As we have already seen, the CLT also applies to averages of quantitative data. A major difference with binary data, for which we know the variance is p(1−p)
, is that with quantitative data we need to estimate the population standard deviation.

In several previous exercises we have illustrated statistical concepts with the unrealistic situation of having access to the entire population. In practice, we do not have access to entire populations. Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations:

We think of X as a random sample from the population of all mice in the control diet and Y
 as a random sample from the population of all mice in the high fat diet.

Define the parameter μx
 as the average of the control population. We estimate this parameter with the sample average X¯
. What is the sample average? **  

txt> dat
   Diet Bodyweight
1  chow      21.51
2  chow      28.14
3  chow      24.04
4  chow      23.45
5  chow      23.68
6  chow      19.79
7  chow      28.40
8  chow      20.98
9  chow      22.51
10 chow      20.10
11 chow      26.91
12 chow      26.25
13   hf      25.71
14   hf      26.37
15   hf      22.80
16   hf      25.34
17   hf      24.97
18   hf      28.14
19   hf      29.58
20   hf      30.92
21   hf      34.02
22   hf      21.90
23   hf      31.53
24   hf      20.73

 ## dat dataset is only a small dataset of 24 variables. this is considered a sample dataset, and can be represent as X or Y 

txt> X <- filter(dat, Diet == "chow")
txt> Y <- filter(dat, Diet == "hf")
txt> X <- filter(dat, Diet == "chow")
txt> X <- filter(dat, Diet == "chow") %>% select(Bodyweight) %>% unlist()
txt> Y <- filter(dat, Diet == "hf") %>% select(Bodyweight) %>% unlist()

 ## parameters μX is the mean of sample X (control), therefore mean(X) 

txt> mean(X)
[1] 23.81333

 ## correction, we can't calculate μx (lower case x = entire population) because we don't have the entire dataset of the population, we only have the sample (dat). we calculated μX above i.e. the average of the control population sample as μX instead of estimate what μx could be 


 ** We don’t know μX
 , but want to use X¯
 to understand μX
. Which of the following uses CLT to understand how well X¯
 approximates μX
 ?
A) X¯
 follows a normal distribution with mean 0 and standard deviation 1.
B) μX
 follows a normal distribution with mean X¯
 and standard deviation σx12√
 where σx
 is the population standard deviation.
C) X¯
 follows a normal distribution with mean μX
 and standard deviation σx
 where σx
 is the population standard deviation.
D) X¯
 follows a normal distribution with mean μX
 and standard deviation σx12√
 where σx
 is the population standard deviation.
** 


 ## ans D: X bar (sample mean) will follow a normal distribution with mean μX and SD defined by sqrt(p*(1-p)/n). It's not B because X bar is a random variable, i.e. not fixed, therefore it makes more sense for X bar to follow a more 'stable' or fixed variable such as the mean of the entire dataset (μX) rather than the other way around. 


 ** The result above tells us the distribution of the following random variable: Z=12‾‾‾√X¯−μXσX
. What does the CLT tell us is the mean of Z
 (you don’t need code)? ** 


 ## with z-score, mean = 0 and sd = 1 

txt> p <- popsd(X)
txt> p
[1] 2.893862
txt> p/sqrt(12)
[1] 0.8353861

 ## dividng popsd / sqrt(n) gives the standard error. the sd (in popsd) is used to see how individual data points vary around the mean, however standard error is used to calculate how sample averages vary around the mean. this would mean taking a small sample size from the bodyweight values of mice > take an average > repeat several times > plot on graph > then use standard error = popsd / sqrt(n) to calculate how far away the sample averages are from the mean  


 **The result of 4 and 5 tell us that we know the distribution of the difference between our estimate and what we want to estimate, but don’t know. However, the equation involves the population standard deviation σX
, which we don’t know. Given what we discussed, what is your estimate of sigmaX? 


 ## sigmaX is the (population) standard deviation. we don't know popsd because we don't have the entire population data, just a sample of 12 mice. therefore, we need to estimate popsd with sd(). this still returns a value with units sigmaX (popsd), but it introduces a lot more uncertainty (bc the SD formula has n-1(?)) to take into account the estimated variability. 


 ## we can't use CLT to estimate because CLT requires that we know popsd (which is very uncommon to have). t-test uses sd() instead because it adapts to only sample datasets, and therefore produces a p-value that also takes into account the uncertainty 


 ## popsd is the true spread of data (but we don't have the entire population data)/ standard error = popsd / sqrt(n). standard error is how far away the sample averages vary away from each other. standard error is also used synonymously with standard deviation? 


 ## that is why in the question it says 'However, the equation involves the population standard deviation σX, which we don’t know.', i.e. you can't calculate CLT without popsd. 


 ## therefore: use popsd() when you have the entire population dataset, but use sd() when you only have the sample dataset. sd() introduces more uncertainty to make an estimation of the true spread in popsd. sd() is also adopted in t-tests when investigating the significance of sample mean values 


 ** Use the CLT to approximate the probability that our estimate X¯
 is off by more than 5.21 ounces from μX ** 


 ## we already established that we cannot use CLT as we do not have popsd(). however, if we choose to use an estimate of popsd() with sd(), we can substitute sd() into the CLT equation. 


 ## the numerator in the z score formula is the 5.21 ounces from the popualtion mean. the numerator is the (sample mean (Xbar) - expected (population) mean). however, we don't know the population mean. so hypothetically, the q is asking what is the probability that the sample mean will be 5.21 ounces from the true mean? i.e. the q has assumed 


 ## therefore, 5.21 acts as the numerator 


 ## standard error sqrt(p(1-p)/n is used for binary data (rolling dice), but for quantitative data we use sd / sqrt(n) (where sd is the sigmaX) 


 ## therefore the updated z score formula is the sample and expected means, divided by the standard error. standard error is sd / sqrt(n), therefore the final z formula = 5.21 / (sd / sqrt(n)) 

txt> 5.21/(sd(X)/sqrt(12))
[1] 5.971126
txt> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7fac73c8f6d0>
<environment: namespace:TeachingDemos>


 ** Now we introduce the concept of a null hypothesis. We don’t know μx
 nor μy
. We want to quantify what the data say about the possibility that the diet has no effect: μx=μy
. If we use CLT, then we approximate the distribution of X¯
 as normal with mean μX
 and standard deviation σX
 and the distribution of Y¯
 as normal with mean μy
 and standard deviation σy
. This implies that the difference Y¯−X¯
 has mean 0
. We described that the standard deviation of this statistic (the standard error) is SE(X¯−Y¯)=σ2y/12+σ2x/12‾‾‾‾‾‾‾‾‾‾‾‾‾√
 and that we estimate the population standard deviations σx
 and σy
 with the sample estimates. What is the estimate of SE(X¯−Y¯)=σ2y/12+σ2x/12‾‾‾‾‾‾‾‾‾‾‾‾‾√
 ? **  

> `?`(`?`(see))
> `?`(se)
No documentation for ‘se’ in specified packages and libraries:
you could try ‘??se’
> `?`(`?`(se))

 ## in a null hypothesis, we are proving that there is no difference between two groups. in this example, the null hypothesis is that the bodyweight between the two control and high-fructose diet mice groups have no difference. this means that the Xbar is equal to Ybar (x = control, y = hf). x/y bar is the sample of X and Y because we do not literally have the entire population, hence the sample 'bar'. in a null hypothesis, we assume that if Xbar and Ybar both follow a normal distribution (i.e. reflective of the real populations X and Y) with meanx and sdy (vice versa with y), that the difference between Xbar and Ybar means are 0 - ie no difference (null hypothesis) 

> url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
> filename <- "femaleMiceWeights.csv"
> if (!file.exists("femaleMiceWeights.csv")) download(url, destfile = filename)
> dat <- read.csv(filename)
> head(dat)
  Diet Bodyweight
1 chow      21.51
2 chow      28.14
3 chow      24.04
4 chow      23.45
5 chow      23.68
6 chow      19.79
> dat
   Diet Bodyweight
1  chow      21.51
2  chow      28.14
3  chow      24.04
4  chow      23.45
5  chow      23.68
6  chow      19.79
7  chow      28.40
8  chow      20.98
9  chow      22.51
10 chow      20.10
11 chow      26.91
12 chow      26.25
13   hf      25.71
14   hf      26.37
15   hf      22.80
16   hf      25.34
17   hf      24.97
18   hf      28.14
19   hf      29.58
20   hf      30.92
21   hf      34.02
22   hf      21.90
23   hf      31.53
24   hf      20.73
> X <- filter(dat, Diet == "chow") %>% select(Bodyweight) %>% unlist()
> head(X)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      21.51       28.14       24.04       23.45       23.68       19.79 
> Y <- filter(dat, Diet == "hf") %>% select(Bodyweight) %>% unlist()
> head(Y)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      25.71       26.37       22.80       25.34       24.97       28.14 

 SE(X¯−Y¯)=σ2y/12+σ2x/12‾‾‾‾‾‾‾‾‾‾‾‾‾√ 


 ## sigma = population variance = popsd() 


 Standard error (SE) = sqrt(p(1-p)/n for binary data, but for quantitative data sd / sqrt(n) 


 ## the question appears to be using the quantitative data formula of sd / sqrt(n),  


 ## the above dat variable is a data sample of a larger population of mice, therefore we cannot use popsd(), and must use sd() to account for the larger variance in the data. 


 ## recap, sigma = standard deviation, sigma squared = variance. the above formula in the q is using sigma squared i.e. variance. however, since the formula is all under a square root, this means that we will need the sd anyway. the n (12) is also already under a sqrt root. therefore, we could do sd / sqrt(12), or, sqrt(variance / n)(?) 

> (sd(X)/sqrt(12)) + (sd(Y)/sqrt(12))
[1] 2.055409

 ## standard error of Xbar Ybar is equal to 2.06 


 ## since X and Y are samples from datasets x and y respectively, X and Y are random variables and will differ every time. therefore, standard error is used to calculate the variability between different sample means (bar = mean). standard deviation describes the variability between each individual observations within the same sample (ie how much variance in the sample from the mean) 


 ** So now we can compute Y¯−X¯
 as well as an estimate of this standard error and construct a t-statistic. What is this t-statistic? ** 


 ## t-statistic is a value that describes if the null is true - taking into account the observed difference between the two samples, divided by the standard error of the two samples. SE is calculated using the above formula, and got 2.06. if the null is true, then there shouldn't be any difference in means betwen the 2 samples anyway. finishing and using this formula: t-statistic = (Ybar - Xbar) / SE(Xbar - Ybar) 

> mean(Y) - mean(X)
[1] 3.020833
> (mean(Y) - mean(X))/2.06
[1] 1.466424

 ## t-statistic. = 1.47 


 ## the t-statistic takes the mean difference / standard error of the 2 samples to produce a *ratio* to describe how many SE the sample mean is from the actual population sd. because sd() is used in the standard error formula (?) 


 ## i.e. t-statistic is used to compare means. 


 ## techincally, the t-statistic numerator should be (Ybar - Xbar) - (μy−μx), where x and y are the entire population dataset, but we don't have the entire population, and we can assume that (μy−μx) = 0 because we are investigating that in the null hypothesis, that there is no difference between them. 


 ** If we apply the CLT, what is the distribution of this t-statistic?
A) Normal with mean 0 and standard deviation 1.
B) t-distributed with 22 degrees of freedom.
C) Normal with mean 0 and standard deviation σ2y/12+σ2x/12‾‾‾‾‾‾‾‾‾‾‾‾‾√
.
D) t-distributed with 12 degrees of freedom. ** 


 to use CLT, we need to know the entire population data - but we don't know. we calculated SE to provide an *estimate* of what the true standard error is. this is because in our standard error estimate we used sd(), which adds variability to take into account that we do not have the entire population. 


 ## ANS: D, degrees of freedom describes the variance (t-distribution). as we do not have the entire population dataset, we used sd().the formula for sd uses n-1 as the denominator, i.e. make a smaller denominator for a larger sd value (i.e. larger standard deviation aka larger variance). degrees of freedom is equal to n-1. therefore, as we are calculating the t-statistic using two samples (X and Y), the total sample size becomes (12-1 + 12-1) = 22 degrees of freedom 


 ** Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed in 10, when the null distribution is true? ** 


 or is answer C?... 


  standard error is calculated using formula: 'SE(X¯−Y¯)=σ2y/12+σ2x/12‾‾‾‾‾‾‾‾‾‾‾‾‾√
 ?'. i instead simplified it by using values i already had, i.e. i did not have variance (sigma squared), but i did have standard deviation (sigma). therefore, i did sd / sqrt(n). 


 ## instead of translating the formmula, i could do var() to calculate sigma squared, this should give the same value as before (2.06) 

> (var(X)/sqrt(12)) + (var(Y)/sqrt(12))
[1] 7.484227
> sqrt(var(X)/(12)) + (var(Y)/(12))
[1] 2.27173
> var
function (x, y = NULL, na.rm = FALSE, use) 
{
    if (missing(use)) 
        use <- if (na.rm) 
            "na.or.complete"
        else "everything"
    na.method <- pmatch(use, c("all.obs", "complete.obs", "pairwise.complete.obs", 
        "everything", "na.or.complete"))
    if (is.na(na.method)) 
        stop("invalid 'use' argument")
    if (is.data.frame(x)) 
        x <- as.matrix(x)
    else if (!is.null(x)) 
        stopifnot(is.atomic(x))
    if (is.data.frame(y)) 
        y <- as.matrix(y)
    else if (!is.null(y)) 
        stopifnot(is.atomic(y))
    .Call(C_cov, x, y, na.method, FALSE)
}
<bytecode: 0x7f928cef0e98>
<environment: namespace:stats>
> `?`(var)

 slight difference why? maybe var() is more accurate, or my other above method was wrong 

> obs <- abs(mean(X) - mean(Y))
> obs
[1] 3.020833
> mean(X) - mean(Y)
[1] -3.020833
> se <- sqrt(var(X)/(12)) + (var(Y)/(12))
> tstat <- obs/se
> tstat
[1] 1.32975

 ## again, the t-statistic numerator should be sample mean differences (obs) - population mean difference. however, we do not have the entire population data. however, when investigating the null hypothesis, we can assume that pop mean difference = 0 


 ## t-statistic (uses the CLT), the null distribution is approximated by normal distribution with means 0 and variance 1. this is because that if the null hypothesis true, the t-statistic behaves like a normal distribution (i.e. no difference between sample means), therefore mean difference between the two groups = 0 (mean=0), and therefore have the same standard deviation (relatively to each other) (sd = 1). since we only have 1 sample from each group, we cannot calculate standard error. however, we can use standard deviation as an estimate to what standard deviation could be. therefore, although CLT uses standard error, we can use sd value as an estimate of se in the equation. 

> `?`(ppnorm)
No documentation for ‘ppnorm’ in specified packages and libraries:
you could try ‘??ppnorm’
> `?`(pnorm)

 ** the null distribution is approximated by normal distribution with means 0 and variance 1 ** 


 ** Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed in 10, when the null distribution is true? ** 


 ## t-statistic is a value that explains the observed difference between the two sample populations, whilst factoring in the noise from the data via standard error. therefore, the lower the difference between the two sample populations, the more supportive it is of the null hypothesis (i.e. no difference), and therefore a lower tstat value. 


 the t-statistic value of 1.33 represents the difference in bodyweight between the two populations of mice on different diets. as above, this tstat value also factors in the noise from the data via standard error. 


 ## note 1.33 does *not* represent bodyweight directly, but rather a standardised unit to measure bodyweight difference, aka 1.33 standard errors 


 ## pnorm() helps explain how likely it would be that we would get 1.33 bodyweight difference between the two diet populations if we took another sample. we currently only have 1 sample from each group available, so we use pnorm() to produce a p-value explaining the probability that < 1.33 

> pnorm(tstat)
[1] 0.9081997

 ## as we are describing the null hypothesis, we do 1 - pnorm(tstat) to get what proportion of values would be above 1.33 

> 1 - pnorm(tstat)
[1] 0.0918003

 ## the null hypothesis explains that there shouldn't be a difference in the mean between the two samples. we used the t-statistics formula to estimate the difference between the means of the two diet samples, factoring in the 'noise' via se. we got a value of 1.33. if there was truly no difference, the t-statistic would have  = 0. if we hypothetically sampled from the two different diet populations again, any t-statistic value < 1.33 would further support the null. however, we want to see the probability that the t-statistic value is *above* > 1.33 - this is because this leans more towards that there is actually a difference between the two different samples (i.e. to reject the null). we get all the values that are > 1.33 by simply doing 1 - pnorm(tstat). this gives a one-sided tailed tests i.e. we're only looking at the higher end of the tail (qnorm distribution plot). we *2 factor in the other tail to get the final p-value to reject the null.  

> (1 - pnorm(tstat)) * 2
[1] 0.1836006

 ## 0.18 > 0.5, therefore we accept the null because the p-value is insigificant. the 0.18 explains that 18% of the time, i would see a difference between the two groups *by random chance*. this is too high of a likelihood to confirm for sure that there is actually a difference.   


 ## however if it were < 0.05, this would be a suitable threshold to explain that there is small enough of a chance i.e.  less than 5% chance that the significant difference I am seeing when comparing the two samples is due to random chance, so it's more acceptable 

> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7f92918a8f20>
<environment: namespace:TeachingDemos>

 **CLT provides an approximation for cases in which the sample size is large. In practice, we can’t check the assumption because we only get to see 1 outcome (which you computed above). As a result, if this approximation is off, so is our p-value. As described earlier, there is another approach that does not require a large sample size, but rather that the distribution of the population is approximately normal. We don’t get to see this distribution so it is again an assumption, although we can look at the distribution of the sample with qqnorm(X) and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows t-distribution. What is the p-value under the t-distribution approximation? Hint: use the t.test function. 


 ## with CLT, the more samples taken / larger sample size, there more closer the data reaches a normal distribution. however, where large sample is not possible, we use t-test. but a t-test assumes a normal distribution of the data 

> url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
> filename <- "femaleMiceWeights.csv"
> if (!file.exists("femaleMiceWeights.csv")) download(url, destfile = filename)
> dat <- read.csv(filename)
> `?`(t.test)
> X <- filter(dat, Diet == "chow") %>% select(Bodyweight) %>% unlist()
> head(X)
Bodyweight1 Bodyweight2 Bodyweight3 Bodyweight4 Bodyweight5 Bodyweight6 
      21.51       28.14       24.04       23.45       23.68       19.79 
> Y <- filter(dat, Diet == "hf") %>% select(Bodyweight) %>% unlist()
> t.test(X, Y)

	Welch Two Sample t-test

data:  X and Y
t = -2.0552, df = 20.236, p-value = 0.053
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.08463229  0.04296563
sample estimates:
mean of x mean of y 
 23.81333  26.83417 

> qqnorm(X)
> qqline(X)
> qqnorm(Y)
> qqline(Y)
> t.test(X)

	One Sample t-test

data:  X
t = 27.292, df = 11, p-value = 1.863e-11
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 21.89290 25.73376
sample estimates:
mean of x 
 23.81333 

> t.test(X, Y)

	Welch Two Sample t-test

data:  X and Y
t = -2.0552, df = 20.236, p-value = 0.053
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.08463229  0.04296563
sample estimates:
mean of x mean of y 
 23.81333  26.83417 


 ## accept null hypothesis: there is no significant difference between hf and control chow diet on mice bodyweight (p > 0.05) 


 ** With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. They can’t both be right. What best describes the difference?
A) A sample size of 12 is not large enough, so we have to use the t-distribution approximation.
B) These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation of the standard error and thus, under the null, large values are more probable under the null distribution.
C) The population data is probably not normally distributed so the t-distribution approximation is wrong.
D) Neither assumption is useful. Both are wrong. ** 


 ## C? difference between CLT and t.test is that we can only assume that the data follows a normal distribution, so if it doesn't, this differs from the CLT p value? 

> `?`(t.test)

 ## ANS: B. t.test considers higher uncertainty in the formula, hence makes the tails of the normal distribution larger, hence 'large values are more probable'. therefore, due to the increased variability and larger tails, this distorts the p-value to be less specific, hence a larger p-value, which means harder to conclude a significant p-value, i.e. more likely for values to fall in the null. 

> txtStop
function() {
  removeTaskCallback('r2txt')
  if( R2txt.vars$closecon ) {
    close( R2txt.vars$con )
  }
  if( R2txt.vars$cmdfile && R2txt.vars$closecon2 ) {
    close( R2txt.vars$con2 )
  }
  options( prompt=R2txt.vars$prompt,
           continue=R2txt.vars$continue )
  if(R2txt.vars$res) {
      sink()
      close(R2txt.vars$outcon)
  }
  evalq( rm(list=ls()), envir=R2txt.vars )
  invisible(NULL)
}
<bytecode: 0x7fdd6d11a628>
<environment: namespace:TeachingDemos>
